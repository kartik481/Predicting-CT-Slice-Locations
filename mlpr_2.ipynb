{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e953564f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of y_train: -9.13868774539957e-15\n",
      "Standard error for y_train (for 5785 entries): 0.011927303389170828\n",
      "Mean of y_val (for 5785 entries): -0.2160085093241599\n",
      "Standard error for y_val (for 5785 entries): 0.01290449880016868\n"
     ]
    }
   ],
   "source": [
    "## Importing the required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "data = np.load('ct_data.npz')  ## Loading the dataset\n",
    "\n",
    "## Slicing the dataset required parts\n",
    "X_train = data['X_train']; X_val = data['X_val']; X_test = data['X_test']\n",
    "y_train = data['y_train']; y_val = data['y_val']; y_test = data['y_test']\n",
    "\n",
    "## Getting standard error \n",
    "print(\"Mean of y_train:\",np.mean(y_train))\n",
    "print(\"Standard error for y_train (for 5785 entries):\",np.std(y_train[:5785], ddof=1)/np.sqrt(len(y_train[:5785])))\n",
    "print(\"Mean of y_val (for 5785 entries):\",np.mean(y_val))\n",
    "print(\"Standard error for y_val (for 5785 entries):\",np.std(y_val, ddof=1)/np.sqrt(len(y_val)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41853e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed Constant columns Indices are: [59, 69, 179, 189, 351]\n",
      "                                          \n",
      "Removed Duplicate column indices are: [354, 195, 76, 77, 185, 283]\n"
     ]
    }
   ],
   "source": [
    "## Data preprocessing\n",
    "\n",
    "## 1st step is to remove Constant columns from the data \n",
    "rm_idx0=[]    \n",
    "\n",
    "for i in range(len(X_train[1])):\n",
    "    col=X_train[:,i]\n",
    "    if all(col[0]==col):\n",
    "        rm_idx0.append(i)\n",
    "\n",
    "## Removing them from training as well from other too\n",
    "X_train = np.delete(X_train,rm_idx0,axis=1)\n",
    "\n",
    "X_val = np.delete(X_val, rm_idx0, axis=1)\n",
    "\n",
    "\n",
    "X_test = np.delete(X_test, rm_idx0, axis=1)\n",
    "\n",
    "print(\"Removed Constant columns Indices are:\", rm_idx0)        \n",
    "\n",
    "## Removing the Duplicate columns\n",
    "\n",
    "indices= np.unique(X_train,return_index=True,axis=1)[1]\n",
    "\n",
    "indices=np.sort(indices)\n",
    "rm_idx=list(set(range(indices[0],indices[-1]+1))-set(indices))\n",
    "\n",
    "## Removing them from training as well from other too\n",
    "X_train = np.delete(X_train, rm_idx,axis=1)\n",
    "\n",
    "X_val = np.delete(X_val, rm_idx, axis=1)\n",
    "\n",
    "\n",
    "X_test = np.delete(X_test, rm_idx, axis=1)\n",
    "\n",
    "## Printing the incides of removed columns\n",
    "print(\"                                          \")\n",
    "print(\"Removed Duplicate column indices are:\",rm_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c18ee684",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining the linear regression using least-square est.\n",
    "\n",
    "def fit_linreg(X, yy, alpha):\n",
    "    k=len(X[1])                                 ## getting number of input features\n",
    "    yy = np.concatenate((yy, np.zeros(k)))      ## adding 0_k to the y_train array\n",
    "    z_k = np.sqrt(alpha) * np.eye(k)\n",
    "    X = np.vstack((X,z_k))  \n",
    "    \n",
    "    b = np.concatenate((np.ones(len(X)-k), np.zeros(k)))[:,None]\n",
    "                                                \n",
    "    X = np.insert(X,[0],b,axis=1)\n",
    "\n",
    "    w_fit = np.linalg.lstsq(X, yy, rcond=None)[0]\n",
    "    \n",
    "    \n",
    "    return w_fit[1:], w_fit[0]                  ## returning the fitted params\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf983918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias is: 0.09105350649797651\n",
      "Weights are: \n",
      " [-6.08615525e-02 -1.06095667e-01  7.53031143e-02  2.78057665e-01\n",
      "  2.59611794e-01  1.19816553e-01  1.05110635e-02  2.32613667e-01\n",
      " -3.17361266e-01 -7.77510933e-02 -4.17318470e-02  4.99788622e-02\n",
      " -2.21837493e-02 -2.47999863e-03 -7.29573201e-03  3.75843616e-02\n",
      "  2.67414563e-02  3.32447940e-03 -8.15353393e-02  1.75498538e-01\n",
      "  7.92614640e-02  2.86048166e-02 -4.09313322e-02 -6.22094524e-03\n",
      " -6.01902872e-02 -5.63195592e-02 -1.47997259e-02 -4.11106517e-01\n",
      "  3.40469070e-01  2.63461215e-01 -1.35315466e-02  1.73477525e-03\n",
      " -9.11983421e-03 -6.98890681e-03  6.51972330e-02 -1.75618988e-01\n",
      " -1.79686338e-01  2.20532723e-01 -9.06442066e-02 -3.28680365e-02\n",
      " -1.80905054e-01  1.91033029e-02 -1.75010207e-01  5.31362120e-02\n",
      " -1.40996396e-01 -2.02005581e-02 -2.04166648e-02  2.13763380e-02\n",
      " -4.53472384e-01  8.70663389e-02 -9.25631074e-02 -3.52428163e-03\n",
      " -7.86791379e-02 -5.89370945e-02 -3.60229412e-02 -8.00712607e-02\n",
      "  4.99929907e-02 -1.32414827e-01  1.16614759e-01 -7.67192217e-02\n",
      "  6.28052858e-02 -1.36156590e-02 -1.24405283e-01 -1.11962320e-01\n",
      " -1.05745595e-01 -1.09096265e-01  1.02784907e-01  1.29260645e-01\n",
      "  1.62385488e-02  8.29137144e-02  1.21588397e-01  2.35635620e-02\n",
      "  6.00428331e-02  5.36065688e-02 -1.58943047e-01 -2.10638769e-01\n",
      "  5.53642172e-02 -4.45653126e-02 -6.35137027e-02  2.30745748e-02\n",
      " -3.49447157e-03 -9.14451236e-02 -1.75856317e-02  1.81592014e-01\n",
      " -1.10382344e-02 -8.85325246e-02  6.53297518e-02 -1.32855425e-01\n",
      "  6.08046976e-02 -4.46882725e-02  6.62615768e-03  3.18639468e-02\n",
      " -1.31828880e-01  3.15130603e-01  4.07034188e-01  2.93057314e-01\n",
      " -8.35626603e-02 -1.86794992e-03  3.90480187e-02  1.23036198e-02\n",
      "  3.61804384e-02  3.92992483e-02  2.22181868e-01  1.64155653e-02\n",
      " -8.38710383e-02 -9.79048545e-02 -1.87848423e-01  3.88340118e-03\n",
      " -3.27803517e-02  6.24800330e-02  2.41386867e-01  1.53100292e-01\n",
      "  1.17882022e-01  5.04847348e-03 -2.78044147e-01 -2.25026918e-01\n",
      " -2.09808590e-01  1.01635052e-01  4.09812789e-02 -5.67527959e-02\n",
      " -7.23040667e-03 -1.30984324e-02 -3.29769036e-02 -1.62741547e-02\n",
      " -1.13666894e-01  4.68472791e-02 -9.78591687e-02  6.50160010e-02\n",
      "  2.24139118e-01 -1.09502517e-01  3.09160555e-02 -1.91953202e-01\n",
      " -9.05907427e-02 -1.42260643e-01 -5.37269731e-02  8.62678637e-02\n",
      " -2.66529808e-02 -6.77824445e-02  6.67732153e-02  1.31781051e-01\n",
      "  2.32616524e-02 -7.07380208e-02 -8.02978017e-02 -5.36813806e-02\n",
      "  1.21452168e-01 -4.12531822e-02 -7.21100203e-02 -9.10778881e-02\n",
      " -9.67135577e-02  6.00292166e-02 -1.88987557e-02  1.31334720e-02\n",
      " -8.05177925e-02 -5.12040306e-03  2.04261416e-02  8.25998913e-02\n",
      " -1.48388899e-02  3.94345952e-02 -1.56402514e-03 -7.55938932e-02\n",
      "  2.00786580e-02 -5.56474326e-02  7.08819738e-02 -1.13715304e-02\n",
      " -3.28839980e-01  1.68364694e-01 -1.17863786e-01 -8.48665832e-03\n",
      " -1.52278253e-02 -2.02244340e-04 -1.04691015e-01  1.23152991e-02\n",
      " -1.51840471e-01 -9.01151212e-02  1.82616179e-01 -1.12551601e-01\n",
      " -2.80053874e-02  1.00012582e-01  5.60763691e-02 -7.07102557e-03\n",
      " -5.71587140e-03 -7.78592943e-02 -1.95761990e-01  1.55958254e-03\n",
      "  1.35107230e-01  1.49494089e-02 -4.16701782e-02  2.88933571e-02\n",
      " -1.08624480e-01 -6.28914888e-02  9.04252121e-03 -3.15405580e-01\n",
      " -5.91950717e-02 -1.71065405e-02 -4.84207895e-02 -1.54606340e-02\n",
      " -1.27313580e-02  2.55973818e-02 -5.12230040e-02  7.79719587e-02\n",
      " -9.59492930e-03 -5.65689090e-02 -4.60056888e-02 -9.92881443e-02\n",
      "  9.34154435e-02  1.38937723e-02  5.55656115e-02 -1.03523060e-01\n",
      " -2.96521999e-03 -1.97373336e-01 -1.15915336e-01  2.45377146e-02\n",
      " -7.51284963e-02  2.95901966e-02  1.59732088e-01 -2.55214932e-02\n",
      " -7.05921972e-02 -1.18534101e-01 -1.28853400e-01 -1.90248379e-01\n",
      " -1.15517031e-01  1.72491018e-01 -1.35081590e-01 -8.82162375e-04\n",
      " -1.75898826e-01  4.37553559e-02 -3.36990974e-02  3.78125873e-02\n",
      " -1.00942340e-01 -1.07945331e-01  1.38563027e-01 -1.08563155e-01\n",
      " -1.77090374e-03  1.44306506e-02 -1.61824091e-01  2.10669305e-02\n",
      " -3.69785996e-02  5.22684779e-02  6.65311216e-01 -7.56237913e-02\n",
      " -2.93882803e-02  4.65750132e-02 -2.07112247e-02  8.17381936e-02\n",
      "  3.18926466e-02 -6.05938774e-03 -2.34145665e-01  2.07264826e-01\n",
      "  2.68748291e-02 -2.75100461e-02  5.75702227e-06  3.39331308e-02\n",
      " -1.02703189e-02 -3.34138144e-02  1.79871085e-01  9.34575330e-03\n",
      " -8.74855308e-02 -1.74237522e-02  6.20731713e-02 -3.16820832e-02\n",
      "  7.91034238e-02  9.30206520e-02 -5.56862037e-02  6.59649028e-02\n",
      " -4.74246784e-02 -2.46496068e-01 -7.55199652e-02 -1.53792392e-02\n",
      " -2.49783440e-02 -1.49770585e-01  1.19841505e-01 -2.03551806e-03\n",
      " -4.89038308e-02 -4.85247748e-02  2.85348752e-02  4.05902677e-02\n",
      " -2.73977734e-03 -5.56445525e-03 -6.31061172e-02  6.18458128e-02\n",
      "  4.28220481e-02  8.96524038e-03  3.33182578e-02  3.09614954e-02\n",
      "  4.99975435e-02  1.72729357e-01  8.62563689e-03  4.96766019e-02\n",
      " -8.92889404e-02  4.42282517e-03  7.11294571e-03  6.54528089e-02\n",
      "  5.17706544e-02 -1.54278426e-01 -2.09825346e-03 -8.85456425e-04\n",
      " -6.48070821e-02  8.69504566e-02  8.66348821e-02 -2.53237348e-02\n",
      " -3.16526561e-02  2.88542919e-01  2.24807405e-01 -2.48728268e-02\n",
      " -4.47132403e-02 -7.82424973e-02  5.09331541e-02  8.25736293e-02\n",
      "  3.41910251e-02  1.02186257e-01  1.43144995e-01 -2.33661646e-02\n",
      "  3.45967092e-02 -3.03613113e-02 -1.39112619e-01  3.05587465e-02\n",
      "  6.83481218e-03 -4.39850740e-02 -1.17374952e-01 -6.91175299e-02\n",
      " -4.47527417e-02 -3.25959504e-02 -3.84450276e-02  2.55009541e-02\n",
      "  1.82002984e-02  6.79272720e-02 -7.68406115e-02 -2.47539196e-02\n",
      " -2.89360719e-02 -3.75559461e-02 -7.37622592e-02  9.46551993e-02\n",
      "  1.57810076e-01 -1.55469957e-01  7.88141999e-02  3.16405169e-02\n",
      "  2.17205686e-02  2.19262730e-02  6.28124114e-02  1.14806498e-01\n",
      "  1.36989292e-01 -6.02040162e-04  6.66557491e-04 -2.80053279e-02\n",
      " -1.18440708e-03 -5.65586184e-03  6.71224631e-02  1.00568659e-01\n",
      " -1.64279314e-01  8.65653643e-03  3.96954653e-02 -1.43297161e-01\n",
      " -4.73488855e-02  6.81528053e-02  1.83899948e-02 -8.73701675e-03\n",
      " -7.72313103e-02 -1.46353459e-02 -1.01985546e-02 -2.68701222e-02\n",
      " -5.40211418e-02  1.14790736e-02 -4.76934694e-02 -2.71385451e-01\n",
      "  1.45368351e-01  1.03736407e-02 -7.82356022e-03 -2.05339687e-02\n",
      "  5.88759483e-02  5.55323179e-02  1.18627774e-01  2.15560819e-01\n",
      "  3.60998227e-02]\n"
     ]
    }
   ],
   "source": [
    "alpha = 30  ## value of lambda\n",
    "\n",
    "ww0, bb0 = fit_linreg(X_train, y_train, alpha)\n",
    "\n",
    "print(\"Bias is:\",bb0)\n",
    "print(\"Weights are:\",\"\\n\",ww0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff525d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Support code\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from scipy.linalg import cho_factor, cho_solve\n",
    "\n",
    "def params_unwrap(param_vec, shapes, sizes):\n",
    "    \"\"\"Helper routine for minimize_list\"\"\"\n",
    "    args = []\n",
    "    pos = 0\n",
    "    for i in range(len(shapes)):\n",
    "        sz = sizes[i]\n",
    "        args.append(param_vec[pos:pos+sz].reshape(shapes[i]))\n",
    "        pos += sz\n",
    "    return args\n",
    "\n",
    "\n",
    "def params_wrap(param_list):\n",
    "    \"\"\"Helper routine for minimize_list\"\"\"\n",
    "    param_list = [np.array(x) for x in param_list]\n",
    "    shapes = [x.shape for x in param_list]\n",
    "    sizes = [x.size for x in param_list]\n",
    "    param_vec = np.zeros(sum(sizes))\n",
    "    pos = 0\n",
    "    for param in param_list:\n",
    "        sz = param.size\n",
    "        param_vec[pos:pos+sz] = param.ravel()\n",
    "        pos += sz\n",
    "    unwrap = lambda pvec: params_unwrap(pvec, shapes, sizes)\n",
    "    return param_vec, unwrap\n",
    "\n",
    "\n",
    "def linreg_cost(params, X, yy, alpha):\n",
    "    \"\"\"Regularized least squares cost function and gradients\n",
    "\n",
    "    Can be optimized with minimize_list -- see fit_linreg_gradopt for a\n",
    "    demonstration.\n",
    "\n",
    "    Inputs:\n",
    "    params: tuple (ww, bb): weights ww (D,), bias bb scalar\n",
    "         X: N,D design matrix of input features\n",
    "        yy: N,  real-valued targets\n",
    "     alpha: regularization constant\n",
    "\n",
    "    Outputs: (E, [ww_bar, bb_bar]), cost and gradients\n",
    "    \"\"\"\n",
    "    # Unpack parameters from list\n",
    "    ww, bb = params\n",
    "\n",
    "    # forward computation of error\n",
    "    ff = np.dot(X, ww) + bb\n",
    "    res = ff - yy\n",
    "    E = np.dot(res, res) + alpha*np.dot(ww, ww)\n",
    "\n",
    "    # reverse computation of gradients\n",
    "    ff_bar = 2*res\n",
    "    bb_bar = np.sum(ff_bar)\n",
    "    ww_bar = np.dot(X.T, ff_bar) + 2*alpha*ww\n",
    "\n",
    "    return E, [ww_bar, bb_bar]\n",
    "\n",
    "def minimize_list(cost, init_list, args):\n",
    "    \"\"\"Optimize a list of arrays (wrapper of scipy.optimize.minimize)\n",
    "\n",
    "    The input function \"cost\" should take a list of parameters,\n",
    "    followed by any extra arguments:\n",
    "        cost(init_list, *args)\n",
    "    should return the cost of the initial condition, and a list in the same\n",
    "    format as init_list giving gradients of the cost wrt the parameters.\n",
    "\n",
    "    The options to the optimizer have been hard-coded. You may wish\n",
    "    to change disp to True to get more diagnostics. You may want to\n",
    "    decrease maxiter while debugging. Although please report all results\n",
    "    in Q2-5 using maxiter=500.\n",
    "    \"\"\"\n",
    "    opt = {'maxiter': 500, 'disp': False}\n",
    "    init, unwrap = params_wrap(init_list)\n",
    "    def wrap_cost(vec, *args):\n",
    "        E, params_bar = cost(unwrap(vec), *args)\n",
    "        vec_bar, _ = params_wrap(params_bar)\n",
    "        return E, vec_bar\n",
    "    res = minimize(wrap_cost, init, args, 'L-BFGS-B', jac=True, options=opt)\n",
    "    return unwrap(res.x)\n",
    "\n",
    "\n",
    "def fit_linreg_gradopt(X, yy, alpha):\n",
    "    \"\"\"\n",
    "    fit a regularized linear regression model with gradient opt\n",
    "\n",
    "         ww, bb = fit_linreg_gradopt(X, yy, alpha)\n",
    "\n",
    "     Find weights and bias by using a gradient-based optimizer\n",
    "     (minimize_list) to improve the regularized least squares cost:\n",
    "\n",
    "       np.sum(((np.dot(X,ww) + bb) - yy)**2) + alpha*np.dot(ww,ww)\n",
    "\n",
    "     Inputs:\n",
    "             X N,D design matrix of input features\n",
    "            yy N,  real-valued targets\n",
    "         alpha     scalar regularization constant\n",
    "\n",
    "     Outputs:\n",
    "            ww D,  fitted weights\n",
    "            bb     scalar fitted bias\n",
    "    \"\"\"\n",
    "    D = X.shape[1]\n",
    "    args = (X, yy, alpha)\n",
    "    init = (np.zeros(D), np.array(0))\n",
    "    ww, bb = minimize_list(linreg_cost, init, args)\n",
    "    return ww, bb\n",
    "\n",
    "def logreg_cost(params, X, yy, alpha):\n",
    "    \"\"\"Regularized logistic regression cost function and gradients\n",
    "\n",
    "    Can be optimized with minimize_list -- see fit_linreg_gradopt for a\n",
    "    demonstration of fitting a similar function.\n",
    "\n",
    "    Inputs:\n",
    "    params: tuple (ww, bb): weights ww (D,), bias bb scalar\n",
    "         X: N,D design matrix of input features\n",
    "        yy: N,  real-valued targets\n",
    "     alpha: regularization constant\n",
    "\n",
    "    Outputs: (E, [ww_bar, bb_bar]), cost and gradients\n",
    "    \"\"\"\n",
    "    # Unpack parameters from list\n",
    "    ww, bb = params\n",
    "\n",
    "    # Force targets to be +/- 1\n",
    "    yy = 2*(yy==1) - 1\n",
    "\n",
    "    # forward computation of error\n",
    "    aa = yy*(np.dot(X, ww) + bb)\n",
    "    sigma = 1/(1 + np.exp(-aa))\n",
    "    E = -np.sum(np.log(sigma)) + alpha*np.dot(ww, ww)\n",
    "\n",
    "    # reverse computation of gradients\n",
    "    aa_bar = sigma - 1\n",
    "    bb_bar = np.dot(aa_bar, yy)\n",
    "    ww_bar = np.dot(X.T, yy*aa_bar) + 2*alpha*ww\n",
    "\n",
    "    return E, (ww_bar, bb_bar)\n",
    "\n",
    "def nn_cost(params, X, yy=None, alpha=None):\n",
    "    \"\"\"NN_COST simple neural network cost function and gradients, or predictions\n",
    "\n",
    "           E, params_bar = nn_cost([ww, bb, V, bk], X, yy, alpha)\n",
    "                    pred = nn_cost([ww, bb, V, bk], X)\n",
    "\n",
    "     Cost function E can be minimized with minimize_list\n",
    "\n",
    "     Inputs:\n",
    "             params (ww, bb, V, bk), where:\n",
    "                    --------------------------------\n",
    "                        ww K,  hidden-output weights\n",
    "                        bb     scalar output bias\n",
    "                         V K,D hidden-input weights\n",
    "                        bk K,  hidden biases\n",
    "                    --------------------------------\n",
    "                  X N,D input design matrix\n",
    "                 yy N,  regression targets\n",
    "              alpha     scalar regularization for weights\n",
    "\n",
    "     Outputs:\n",
    "                     E  sum of squares error\n",
    "            params_bar  gradients wrt params, same format as params\n",
    "     OR\n",
    "               pred N,  predictions if only params and X are given as inputs\n",
    "    \"\"\"\n",
    "    # Unpack parameters from list\n",
    "    ww, bb, V, bk = params\n",
    "\n",
    "    # Forwards computation of cost\n",
    "    A = np.dot(X, V.T) + bk[None,:] # N,K\n",
    "    P = 1 / (1 + np.exp(-A)) # N,K\n",
    "    F = np.dot(P, ww) + bb # N,\n",
    "    if yy is None:\n",
    "        # user wants prediction rather than training signal:\n",
    "        return F\n",
    "    res = F - yy # N,\n",
    "    E = np.dot(res, res) + alpha*(np.sum(V*V) + np.dot(ww,ww)) # 1x1\n",
    "\n",
    "    # Reverse computation of gradients\n",
    "    F_bar = 2*res # N,\n",
    "    ww_bar = np.dot(P.T, F_bar) + 2*alpha*ww # K,\n",
    "    bb_bar = np.sum(F_bar) # scalar\n",
    "    P_bar = np.dot(F_bar[:,None], ww[None,:]) # N,K\n",
    "    A_bar = P_bar * P * (1 - P) # N,K\n",
    "    V_bar = np.dot(A_bar.T, X) + 2*alpha*V # K,D\n",
    "    bk_bar = np.sum(A_bar, 0)\n",
    "\n",
    "    return E, (ww_bar, bb_bar, V_bar, bk_bar)\n",
    "\n",
    "def rbf_fn(X1, X2):\n",
    "    \"\"\"Helper routine for gp_post_par\"\"\"\n",
    "    return np.exp((np.dot(X1,(2*X2.T))-np.sum(X1*X1,1)[:,None]) - np.sum(X2*X2,1)[None,:])\n",
    "\n",
    "\n",
    "def gauss_kernel_fn(X1, X2, ell, sigma_f):\n",
    "    \"\"\"Helper routine for gp_post_par\"\"\"\n",
    "    return sigma_f**2 * rbf_fn(X1/(np.sqrt(2)*ell), X2/(np.sqrt(2)*ell))\n",
    "\n",
    "\n",
    "def gp_post_par(X_rest, X_obs, yy, sigma_y=0.05, ell=5.0, sigma_f=0.1):\n",
    "    \"\"\"GP_POST_PAR means and covariances of a posterior Gaussian process\n",
    "\n",
    "         rest_cond_mu, rest_cond_cov = gp_post_par(X_rest, X_obs, yy)\n",
    "         rest_cond_mu, rest_cond_cov = gp_post_par(X_rest, X_obs, yy, sigma_y, ell, sigma_f)\n",
    "\n",
    "     Calculate the means and covariances at all test locations of the posterior Gaussian\n",
    "     process conditioned on the observations yy at observed locations X_obs.\n",
    "\n",
    "     Inputs:\n",
    "                 X_rest GP test locations\n",
    "                  X_obs locations of observations\n",
    "                     yy observed values\n",
    "                sigma_y observation noise standard deviation\n",
    "                    ell kernel function length scale\n",
    "                sigma_f kernel function standard deviation\n",
    "\n",
    "     Outputs:\n",
    "           rest_cond_mu mean at each location in X_rest\n",
    "          rest_cond_cov covariance matrix between function values at all test locations\n",
    "    \"\"\"\n",
    "    X_rest = X_rest[:, None]\n",
    "    X_obs = X_obs[:, None]\n",
    "    K_rest = gauss_kernel_fn(X_rest, X_rest, ell, sigma_f)\n",
    "    K_rest_obs = gauss_kernel_fn(X_rest, X_obs, ell, sigma_f)\n",
    "    K_obs = gauss_kernel_fn(X_obs, X_obs, ell, sigma_f)\n",
    "    M = K_obs + sigma_y**2 * np.eye(yy.size)\n",
    "    M_cho, M_low = cho_factor(M)\n",
    "    rest_cond_mu = np.dot(K_rest_obs, cho_solve((M_cho, M_low), yy))\n",
    "    rest_cond_cov = K_rest - np.dot(K_rest_obs, cho_solve((M_cho, M_low), K_rest_obs.T))\n",
    "\n",
    "    return rest_cond_mu, rest_cond_cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1262a835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias:  0.08983942508643088 \n",
      "\n",
      "weights:  [-6.08928794e-02 -1.06109231e-01  7.53199790e-02  2.78058745e-01\n",
      "  2.59650357e-01  1.19767171e-01  1.05065679e-02  2.32640328e-01\n",
      " -3.17410095e-01 -7.79395725e-02 -4.16863910e-02  4.99515149e-02\n",
      " -2.22130095e-02 -2.47116900e-03 -7.34062422e-03  3.76237659e-02\n",
      "  2.67685541e-02  3.28086014e-03 -8.14198810e-02  1.75633257e-01\n",
      "  7.92432171e-02  2.86749071e-02 -4.09596766e-02 -6.22108255e-03\n",
      " -6.01534646e-02 -5.63056508e-02 -1.47844013e-02 -4.10930801e-01\n",
      "  3.40022329e-01  2.63335827e-01 -1.35872609e-02  1.66221922e-03\n",
      " -9.11711030e-03 -7.00848604e-03  6.51838705e-02 -1.75613930e-01\n",
      " -1.79693155e-01  2.20564235e-01 -9.07051362e-02 -3.30882147e-02\n",
      " -1.80930149e-01  1.91309155e-02 -1.74997057e-01  5.31853600e-02\n",
      " -1.40982937e-01 -2.01732057e-02 -2.04407470e-02  2.17917797e-02\n",
      " -4.51780456e-01  8.75602083e-02 -9.25508965e-02 -3.47882886e-03\n",
      " -7.86813533e-02 -5.90120793e-02 -3.59756491e-02 -8.00154974e-02\n",
      "  5.00354796e-02 -1.32535849e-01  1.16684096e-01 -7.66211791e-02\n",
      "  6.28223777e-02 -1.36229313e-02 -1.24330837e-01 -1.11965530e-01\n",
      " -1.05871183e-01 -1.08969216e-01  1.02886867e-01  1.28952656e-01\n",
      "  1.61689458e-02  8.28466846e-02  1.21645961e-01  2.34963112e-02\n",
      "  6.00149070e-02  5.35866904e-02 -1.58918008e-01 -2.10641364e-01\n",
      "  5.54141259e-02 -4.45863056e-02 -6.35148087e-02  2.30299632e-02\n",
      " -3.48423724e-03 -9.14796883e-02 -1.77799288e-02  1.81631473e-01\n",
      " -1.10361931e-02 -8.83894649e-02  6.53633872e-02 -1.32857595e-01\n",
      "  6.07991204e-02 -4.46443952e-02  6.61710232e-03  3.18716000e-02\n",
      " -1.31928383e-01  3.15312011e-01  4.07418423e-01  2.93765935e-01\n",
      " -8.36635563e-02 -1.91141343e-03  3.90614945e-02  1.23188395e-02\n",
      "  3.61823948e-02  3.92709528e-02  2.22194561e-01  1.63853835e-02\n",
      " -8.40181281e-02 -9.78930805e-02 -1.87805464e-01  3.98960900e-03\n",
      " -3.28181029e-02  6.24754977e-02  2.41359743e-01  1.53132933e-01\n",
      "  1.17871715e-01  5.04771145e-03 -2.77936284e-01 -2.24971005e-01\n",
      " -2.09794412e-01  1.01600861e-01  4.10031287e-02 -5.67779475e-02\n",
      " -7.22616851e-03 -1.30914257e-02 -3.29799233e-02 -1.63048724e-02\n",
      " -1.13703997e-01  4.67278344e-02 -9.78534472e-02  6.50122713e-02\n",
      "  2.24133488e-01 -1.09481916e-01  3.08966960e-02 -1.91934583e-01\n",
      " -9.05709762e-02 -1.42310729e-01 -5.37337873e-02  8.61357226e-02\n",
      " -2.65850257e-02 -6.77776969e-02  6.67895029e-02  1.31791402e-01\n",
      "  2.32979093e-02 -7.07463407e-02 -8.02807016e-02 -5.37762761e-02\n",
      "  1.21732669e-01 -4.13503869e-02 -7.21680826e-02 -9.11027951e-02\n",
      " -9.66936396e-02  6.00376469e-02 -1.89070371e-02  1.31356759e-02\n",
      " -8.06093542e-02 -5.16988568e-03  2.04767679e-02  8.26309211e-02\n",
      " -1.48418297e-02  3.94584807e-02 -1.56678148e-03 -7.56326185e-02\n",
      "  2.00845756e-02 -5.56619139e-02  7.09943147e-02 -1.12255194e-02\n",
      " -3.29669750e-01  1.67421587e-01 -1.17864799e-01 -8.46771638e-03\n",
      " -1.52475024e-02 -1.90052668e-04 -1.04653438e-01  1.23940147e-02\n",
      " -1.52037061e-01 -8.98563978e-02  1.81387967e-01 -1.12574496e-01\n",
      " -2.80129229e-02  1.00026351e-01  5.61223944e-02 -7.11505360e-03\n",
      " -5.68420193e-03 -7.74719007e-02 -1.95968771e-01  1.63640032e-03\n",
      "  1.35094037e-01  1.49504877e-02 -4.16632971e-02  2.89062827e-02\n",
      " -1.08661360e-01 -6.31554403e-02  8.79593748e-03 -3.16315189e-01\n",
      " -5.92186461e-02 -1.70756520e-02 -4.84292061e-02 -1.54408150e-02\n",
      " -1.27732925e-02  2.56160160e-02 -5.11524069e-02  7.80299936e-02\n",
      " -9.56880988e-03 -5.67617019e-02 -4.59576807e-02 -9.93349883e-02\n",
      "  9.34475794e-02  1.38667597e-02  5.56113997e-02 -1.03531343e-01\n",
      " -2.93211618e-03 -1.97441247e-01 -1.15997124e-01  2.44541663e-02\n",
      " -7.51702534e-02  2.95928810e-02  1.59707577e-01 -2.55645697e-02\n",
      " -7.06091726e-02 -1.18534186e-01 -1.28838357e-01 -1.90195067e-01\n",
      " -1.15565059e-01  1.72652918e-01 -1.35073240e-01 -8.56864615e-04\n",
      " -1.75883560e-01  4.37668180e-02 -3.36673225e-02  3.78182948e-02\n",
      " -1.00931194e-01 -1.07940908e-01  1.38558665e-01 -1.08572560e-01\n",
      " -1.79511559e-03  1.44355410e-02 -1.61822140e-01  2.10474878e-02\n",
      " -3.69732171e-02  5.22699748e-02  6.64909913e-01 -7.47038569e-02\n",
      " -2.93728165e-02  4.65758235e-02 -2.07228618e-02  8.17312169e-02\n",
      "  3.18947744e-02 -6.11982123e-03 -2.34721468e-01  2.06611926e-01\n",
      "  2.68506259e-02 -2.75091743e-02 -3.48280798e-06  3.39604431e-02\n",
      " -1.02779070e-02 -3.33938149e-02  1.79777128e-01  9.48707445e-03\n",
      " -8.74980090e-02 -1.74479283e-02  6.20921204e-02 -3.16867501e-02\n",
      "  7.90929129e-02  9.30273513e-02 -5.54476368e-02  6.58264757e-02\n",
      " -4.73942402e-02 -2.46474211e-01 -7.55331041e-02 -1.53875016e-02\n",
      " -2.50063552e-02 -1.49699414e-01  1.20108136e-01 -4.77789539e-03\n",
      " -4.88811875e-02 -4.85334881e-02  2.85253271e-02  4.05814574e-02\n",
      " -2.72337403e-03 -5.66845176e-03 -6.32060415e-02  6.18328789e-02\n",
      "  4.28298591e-02  8.97347269e-03  3.32842948e-02  3.09666006e-02\n",
      "  5.00507020e-02  1.72908221e-01  8.79738273e-03  4.96899373e-02\n",
      " -8.92533531e-02  4.40648834e-03  7.12893697e-03  6.54679468e-02\n",
      "  5.17919811e-02 -1.55278377e-01 -1.87237624e-03 -9.01853905e-04\n",
      " -6.48358705e-02  8.69620364e-02  8.66368357e-02 -2.53344188e-02\n",
      " -3.16352275e-02  2.88521407e-01  2.24127167e-01 -2.48469962e-02\n",
      " -4.47598621e-02 -7.82063149e-02  5.08826866e-02  8.25922441e-02\n",
      "  3.41387810e-02  1.02239017e-01  1.43418218e-01 -2.33768788e-02\n",
      "  3.45930298e-02 -3.03579448e-02 -1.39134993e-01  3.05424015e-02\n",
      "  6.88297296e-03 -4.40229366e-02 -1.17080671e-01 -6.91131214e-02\n",
      " -4.47953396e-02 -3.25878095e-02 -3.84207327e-02  2.55475152e-02\n",
      "  1.82217783e-02  6.74735465e-02 -7.71769105e-02 -2.47607871e-02\n",
      " -2.89177010e-02 -3.75375113e-02 -7.37685252e-02  9.46366055e-02\n",
      "  1.58012317e-01 -1.55653204e-01  7.90452110e-02  3.16318438e-02\n",
      "  2.17404690e-02  2.19214597e-02  6.29257190e-02  1.14764089e-01\n",
      "  1.36851365e-01 -5.87864813e-04  6.81254192e-04 -2.80062532e-02\n",
      " -1.20040957e-03 -5.66737185e-03  6.71045887e-02  1.00646611e-01\n",
      " -1.64542055e-01  8.64802932e-03  3.97164979e-02 -1.43265415e-01\n",
      " -4.73988885e-02  6.81827024e-02  1.83255032e-02 -8.76705826e-03\n",
      " -7.74902347e-02 -1.46460512e-02 -1.02048296e-02 -2.69252301e-02\n",
      " -5.39942944e-02  1.14491401e-02 -4.77314306e-02 -2.71609767e-01\n",
      "  1.45860808e-01  1.03808349e-02 -7.78729778e-03 -2.05181519e-02\n",
      "  5.89063311e-02  5.55299237e-02  1.18652613e-01  2.15888521e-01\n",
      "  3.53028113e-02]\n"
     ]
    }
   ],
   "source": [
    "## calling the gradient descent method to compare\n",
    "ww1,bb1 = fit_linreg_gradopt(X_train, y_train, 30)\n",
    "\n",
    "print(\"Bias: \",bb1,\"\\n\")\n",
    "print(\"weights: \",ww1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc55291c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining the root mean square cost function \n",
    "def rmse(pred,yy):\n",
    "    return np.sqrt(np.mean((pred-yy)**2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99418f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root means square for training set(using least square method): 0.3567565397204054\n",
      "Root means square for validation set(using least square method): 0.4230521968394701\n"
     ]
    }
   ],
   "source": [
    "## Calulating the errors for least square method\n",
    "pred1_train = np.dot(X_train,ww0)+bb0\n",
    "pred2_val = np.dot(X_val,ww0)+bb0\n",
    "print(\"Root means square for training set(using least square method):\",rmse(pred1_train, y_train))\n",
    "print(\"Root means square for validation set(using least square method):\",rmse(pred2_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b126c2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root means square for training set(using least square method): 0.3567569385517838\n",
      "Root means square for validation set(using least square method): 0.4230540100048547\n"
     ]
    }
   ],
   "source": [
    "# Calulating the errors for gradient method\n",
    "pred1_train = np.dot(X_train,ww1)+bb1\n",
    "pred2_val = np.dot(X_val,ww1)+bb1\n",
    "print(\"Root means square for training set(using least square method):\",rmse(pred1_train, y_train))\n",
    "print(\"Root means square for validation set(using least square method):\",rmse(pred2_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab04f282",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_logreg_gradopt(X, yy, alpha):\n",
    "    \"\"\"\n",
    "    fit a regularized linear regression model with gradient opt\n",
    "\n",
    "         ww, bb = fit_linreg_gradopt(X, yy, alpha)\n",
    "\n",
    "     Find weights and bias by using a gradient-based optimizer\n",
    "     (minimize_list) to improve the regularized least squares cost:\n",
    "\n",
    "       np.sum(((np.dot(X,ww) + bb) - yy)**2) + alpha*np.dot(ww,ww)\n",
    "\n",
    "     Inputs:\n",
    "             X N,D design matrix of input features\n",
    "            yy N,  real-valued targets\n",
    "         alpha     scalar regularization constant\n",
    "\n",
    "     Outputs:\n",
    "            ww D,  fitted weights\n",
    "            bb     scalar fitted bias\n",
    "    \"\"\"\n",
    "    D = X.shape[1]\n",
    "    args = (X, yy, alpha)\n",
    "    init = (np.zeros(D), np.array(0))\n",
    "    ww, bb = minimize_list(logreg_cost, init, args)\n",
    "    return ww, bb\n",
    "\n",
    "\n",
    "K = 20 ## number of thresholded classification problems to fit\n",
    "\n",
    "## Setting the threshold \n",
    "mx = np.max(y_train); mn = np.min(y_train); hh = (mx-mn)/(K+1)\n",
    "thresholds = np.linspace(mn+hh, mx-hh, num=K, endpoint=True)  \n",
    "\n",
    "## Creating an array to store weights and biases \n",
    "w_fit2= np.array([[0.0]* (len(X_train[1])+1)] * K)\n",
    "\n",
    "## getting the weights and biases for K problems\n",
    "for kk in range(K):\n",
    "    labels = y_train > thresholds[kk]\n",
    "    ww2, bb2 = fit_logreg_gradopt(X_train, labels, alpha=30)\n",
    "    w_fit2[kk,0] = bb2\n",
    "    w_fit2[kk,1:]=ww2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e1ca273",
   "metadata": {},
   "outputs": [],
   "source": [
    "## getting the parameter values \n",
    "bb2_hat = w_fit2[:,0]\n",
    "ww2_hat = w_fit2[:,1:]\n",
    "\n",
    "## Defining a sigmoid function\n",
    "def sigmoid(a):\n",
    "    return 1 / (1+np.exp(-a))\n",
    "\n",
    "\n",
    "## getting the new probabilities for train and validation sets \n",
    "X_train_new = sigmoid(np.dot(X_train, np.transpose(ww2_hat))+bb2_hat) \n",
    "X_val_new = sigmoid(np.dot(X_val, np.transpose(ww2_hat))+bb2_hat)\n",
    "\n",
    "\n",
    "## Fitting values using the least square estimator again\n",
    "nn_ww, nn_bb = fit_linreg(X_train_new, y_train, alpha=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "546bbdc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root means square for training set: 0.15441150430439968\n",
      "Root means square for validation set: 0.2542477298370707\n"
     ]
    }
   ],
   "source": [
    "## Getting the prediction for updated parameters\n",
    "pred1_train = np.dot(X_train_new, nn_ww) + nn_bb\n",
    "pred2_val = np.dot(X_val_new, nn_ww) + nn_bb\n",
    "\n",
    "print(\"Root means square for training set:\",rmse(pred1_train, y_train))\n",
    "print(\"Root means square for validation set:\",rmse(pred2_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72bafe6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting the seed for current session\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "## Defining a function to fit a Neural network\n",
    "def fit_nn_gradopt(X, yy, K, alpha, w_random = True):\n",
    "    \"\"\"\n",
    "    fit a regularized linear regression model with gradient opt\n",
    "\n",
    "         ww, bb = fit_linreg_gradopt(X, yy, alpha)\n",
    "\n",
    "     Find weights and bias by using a gradient-based optimizer\n",
    "     (minimize_list) to improve the regularized least squares cost:\n",
    "\n",
    "       np.sum(((np.dot(X,ww) + bb) - yy)**2) + alpha*np.dot(ww,ww)\n",
    "\n",
    "     Inputs:\n",
    "             X N,D design matrix of input features\n",
    "            yy N,  real-valued targets\n",
    "         alpha     scalar regularization constant\n",
    "\n",
    "     Outputs:\n",
    "            ww D,  fitted weights\n",
    "            bb     scalar fitted bias\n",
    "    \"\"\"\n",
    "    args = (X, yy, alpha)  ## Storing values in a tuple\n",
    "    \n",
    "    ## if we want to fit using random initialized weights\n",
    "    if w_random:\n",
    "        D = len(X_train[1])\n",
    "        # calculate the range for the weights\n",
    "        l = len(X_train[1]) \n",
    "    \n",
    "        # generate random numbers for weights\n",
    "        ww = 0.1 * np.random.randn(K) / np.sqrt(K)\n",
    "        V = 0.1 * np.random.randn(K,D)/ np.sqrt(D)\n",
    "        \n",
    "        ## Biases should be zero\n",
    "        bk = np.zeros(K)\n",
    "        bb = 0\n",
    "        \n",
    "        init = (ww, bb, V, bk)\n",
    "        ww, bb, V, bk = minimize_list(nn_cost, init, args)\n",
    "        return (ww, bb, V, bk)\n",
    "    \n",
    "    else:\n",
    "        init = (nn_ww,nn_bb,ww2_hat, bb2_hat)    ## Initialization from the results we obtained above cells\n",
    "        ww, bb, V, bk = minimize_list(nn_cost, init, args)\n",
    "        return (ww, bb, V, bk)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "14b003ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set RMSE for NN(with random initialization): 0.13698780122060805\n",
      "Validation set RMSE for NN(with random initialization): 0.26538584109603225\n"
     ]
    }
   ],
   "source": [
    "## Getting the fitted parameters using random initialization\n",
    "params = fit_nn_gradopt(X_train, y_train,K=20, alpha=30)\n",
    "\n",
    "## Getting predictions for train & validation sets\n",
    "pred_train_nn = nn_cost(params, X_train, yy=None, alpha=30)\n",
    "pred_val_nn = nn_cost(params, X_val, yy=None, alpha=30)\n",
    "print(\"Training set RMSE for NN(with random initialization):\", rmse(pred_train_nn, y_train))\n",
    "print(\"Validation set RMSE for NN(with random initialization):\", rmse(pred_val_nn, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "608d25ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set RMSE for NN: 0.13972707083230795\n",
      "Validation set RMSE for NN: 0.26789560258337225\n"
     ]
    }
   ],
   "source": [
    "## Getting the fitted parameters without using random initialization\n",
    "params2 = fit_nn_gradopt(X_train, y_train,K=20, alpha=30, w_random = False)\n",
    "\n",
    "## Getting predictions for train & validation sets\n",
    "pred1_train_nn = nn_cost(params2, X_train, yy=None, alpha=30)\n",
    "pred1_val_nn = nn_cost(params2, X_val, yy=None, alpha=30)\n",
    "\n",
    "print(\"Training set RMSE for NN:\", rmse(pred1_train_nn, y_train))\n",
    "print(\"Validation set RMSE for NN:\", rmse(pred1_val_nn, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9778ac0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation RMSE for the value of alpha 8.92 => 0.2601516227720478\n",
      "Validation RMSE for the value of alpha 24.78 => 0.2696628649925378\n",
      "Validation RMSE for the value of alpha 15.44 => 0.25991405180552374\n"
     ]
    }
   ],
   "source": [
    "## function defined to train NN for specific values of alpha\n",
    "def train_nn_reg(X_train, X_val, yy, y_val, train_alpha):\n",
    "    \n",
    "    param = fit_nn_gradopt(X_train, yy, K=20, alpha= train_alpha)\n",
    "    \n",
    "    pred_val = nn_cost(param, X_val, yy=None, alpha= train_alpha)\n",
    "\n",
    "    return (rmse(pred_val,y_val), param)  ## returning RMSE & fitted parameters\n",
    "    \n",
    "## Defining a range values of alpha's to choose from \n",
    "alpha= np.arange(0,50,0.02)   \n",
    "\n",
    "## getting the indicies for randomly selected values \n",
    "indicies = np.random.choice(len(alpha),3) \n",
    "\n",
    "## Putting them in a already observed values\n",
    "obs_alpha = np.array(alpha[indicies])\n",
    "\n",
    "## Getting values of alpha to test\n",
    "test_alpha = np.delete(alpha,indicies)\n",
    "\n",
    "## getting the RMSE for observed values\n",
    "obs_alpha_val = np.array([])\n",
    "for alpha in obs_alpha: \n",
    "    val_rmse = train_nn_reg(X_train, X_val, y_train, y_val, alpha)[0]\n",
    "    obs_alpha_val = np.append(obs_alpha_val, val_rmse )\n",
    "    print(\"Validation RMSE for the value of alpha {0} => {1}\".format(alpha,\n",
    "       val_rmse))\n",
    "          \n",
    "    \n",
    "\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "60e9900b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum probability for Alpha(=12.38) is 0.4957708365024785, Validation RMSE => 0.2650949277648916\n",
      "Maximum probability for Alpha(=4.96) is 0.45218243927731866, Validation RMSE => 0.2508676456097986\n",
      "Maximum probability for Alpha(=2.7800000000000002) is 0.4069772196565155, Validation RMSE => 0.2496093182383833\n",
      "Maximum probability for Alpha(=2.34) is 0.41801510111029233, Validation RMSE => 0.23065023115091857\n",
      "Maximum probability for Alpha(=0.0) is 0.16521437452495163, Validation RMSE => 0.25922174997459124\n",
      "Maximum probability for Alpha(=49.980000000000004) is 0.08033464746951624, Validation RMSE => 0.280199754230829\n",
      "Maximum probability for Alpha(=36.6) is 0.07664688603151762, Validation RMSE => 0.2785540428735084\n",
      "Maximum probability for Alpha(=43.22) is 0.022153485643845148, Validation RMSE => 0.2784977084805509\n",
      "Maximum probability for Alpha(=30.34) is 0.017782503302289965, Validation RMSE => 0.27051755101601144\n",
      "Maximum probability for Alpha(=19.56) is 0.015045885844013005, Validation RMSE => 0.2673312190908119\n"
     ]
    }
   ],
   "source": [
    "## using Bayesian optimization to find best values of alpha(lambda)\n",
    "\n",
    "## importing scipy\n",
    "import scipy.stats\n",
    "\n",
    "\n",
    "## Taking base value as Validation rmse if weights are randomized\n",
    "log_base_rmse = np.log(rmse(pred_val_nn, y_val))    \n",
    "\n",
    "## Subtracting the base from observed RMSE(alphas)\n",
    "y = np.array(log_base_rmse - np.log(obs_alpha_val))\n",
    "\n",
    "## Getting posterior mean and covarinaces \n",
    "post_mean, post_cov  = gp_post_par(test_alpha, obs_alpha, y)\n",
    "\n",
    "## getting the Standard deviation\n",
    "post_std = np.sqrt(np.diag(post_cov))         \n",
    "\n",
    "## Defining Probability acquistion function\n",
    "def phi(post_mean, post_std, y):\n",
    "    return scipy.stats.norm.cdf((post_mean - max(y)) / post_std)\n",
    "\n",
    "## Initilzing the parameters to extract\n",
    "best_alpha = 0.0\n",
    "best_alpha_rmse = 9999.0\n",
    "best_params = set()\n",
    "\n",
    "## Running for 10 alpha values\n",
    "for _ in range(10):\n",
    "    ## getting the maximum prob. for specific alpha \n",
    "    prob_max = phi(post_mean, post_std, y)\n",
    "    \n",
    "    ## getting it's index\n",
    "    idx = np.argmax(prob_max)\n",
    "    \n",
    "    ## Calculating it's RMSE to compare and treat as prior(observed)\n",
    "    alpha_val_rmse, params = train_nn_reg(X_train, X_val, y_train, y_val, test_alpha[idx])\n",
    "    \n",
    "    ## Selecting the best parameters from whole iteration\n",
    "    if  alpha_val_rmse < best_alpha_rmse:\n",
    "        best_alpha = test_alpha[idx]\n",
    "        best_alpha_rmse = alpha_val_rmse\n",
    "        best_params = params\n",
    "    \n",
    "    print(\"Maximum probability for Alpha(={0}) is {1}, Validation RMSE => {2}\".format( \n",
    "                                test_alpha[idx], prob_max[idx], alpha_val_rmse))\n",
    "    \n",
    "    ## appending the observed RMSE values for specific alpha \n",
    "    obs_alpha_val = np.append(obs_alpha_val, alpha_val_rmse)\n",
    "    \n",
    "    ## appending the observed alpha values \n",
    "    obs_alpha = np.append(obs_alpha, test_alpha[idx])\n",
    "    \n",
    "    ## deleting them from values to be tested \n",
    "    test_alpha = np.delete(test_alpha,idx)\n",
    "    \n",
    "    ## Subtracting the base from observed RMSE(alphas)\n",
    "    y = np.array(log_base_rmse - np.log(obs_alpha_val))\n",
    "    \n",
    "    ## Getting posterior mean and covarinaces\n",
    "    post_mean, post_cov  = gp_post_par(test_alpha, obs_alpha, y)\n",
    "    \n",
    "    ## getting standard deviation \n",
    "    post_std = np.sqrt(np.diag(post_cov))\n",
    "   \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ce61cd80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Best value for alpha is 2.34\n",
      "The Validation error is 0.23065023115091857\n",
      "The Test error is 0.2703016646642553\n"
     ]
    }
   ],
   "source": [
    "## Traning on best alpha to get test error\n",
    "pred_test = nn_cost(best_params, X_test, yy=None, alpha=best_alpha)   ## Prediction for test set\n",
    "test_error = rmse(pred_test, y_test)\n",
    "\n",
    "print(\"The Best value for alpha is {0}\".format(best_alpha))\n",
    "print(\"The Validation error is {0}\".format(best_alpha_rmse))\n",
    "print(\"The Test error is {0}\".format(test_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4922800-fbfa-4ee4-9289-3cea34b6295d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
