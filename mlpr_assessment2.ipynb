{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e953564f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of y_train: -9.13868774539957e-15\n",
      "Standard error for y_train (for 5785 entries): 0.011927303389170828\n",
      "Mean of y_val (for 5785 entries): -0.2160085093241599\n",
      "Standard error for y_val (for 5785 entries): 0.01290449880016868\n"
     ]
    }
   ],
   "source": [
    "##Assessment 2 MLPR\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "data = np.load('ct_data.npz')\n",
    "X_train = data['X_train']; X_val = data['X_val']; X_test = data['X_test']\n",
    "y_train = data['y_train']; y_val = data['y_val']; y_test = data['y_test']\n",
    "\n",
    "########################## 1st question ####################################### \n",
    "print(\"Mean of y_train:\",np.mean(y_train))\n",
    "print(\"Standard error for y_train (for 5785 entries):\",np.std(y_train[:5785], ddof=1)/np.sqrt(len(y_train[:5785])))\n",
    "print(\"Mean of y_val (for 5785 entries):\",np.mean(y_val))\n",
    "print(\"Standard error for y_val (for 5785 entries):\",np.std(y_val, ddof=1)/np.sqrt(len(y_val)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41853e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed Constant columns Indices are: [59, 69, 179, 189, 351]\n",
      "                                          \n",
      "Removed Duplicate column indices are: [354, 195, 76, 77, 185, 283]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## Constants\n",
    "\n",
    "rm_idx0=[]    \n",
    "\n",
    "for i in range(len(X_train[1])):\n",
    "    col=X_train[:,i]\n",
    "    if all(col[0]==col):\n",
    "        rm_idx0.append(i)\n",
    "        \n",
    "X_train = np.delete(X_train,rm_idx0,axis=1)\n",
    "\n",
    "X_val = np.delete(X_val, rm_idx0, axis=1)\n",
    "\n",
    "\n",
    "X_test = np.delete(X_test, rm_idx0, axis=1)\n",
    "\n",
    "print(\"Removed Constant columns Indices are:\", rm_idx0)        \n",
    "\n",
    "## Duplicates\n",
    "\n",
    "indices= np.unique(X_train,return_index=True,axis=1)[1]\n",
    "\n",
    "indices=np.sort(indices)\n",
    "rm_idx=list(set(range(indices[0],indices[-1]+1))-set(indices))\n",
    "\n",
    "X_train = np.delete(X_train, rm_idx,axis=1)\n",
    "\n",
    "X_val = np.delete(X_val, rm_idx, axis=1)\n",
    "\n",
    "\n",
    "X_test = np.delete(X_test, rm_idx, axis=1)\n",
    "print(\"                                          \")\n",
    "print(\"Removed Duplicate column indices are:\",rm_idx)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c18ee684",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2nd question\n",
    "\n",
    "\n",
    "\n",
    "def fit_linreg(X, yy, alpha):\n",
    "    k=len(X[1])                                 ## getting number of input features\n",
    "    yy = np.concatenate((yy, np.zeros(k)))      ## adding 0_k to the y_train array\n",
    "    z_k = np.sqrt(alpha) * np.eye(k)\n",
    "    X = np.vstack((X,z_k))  \n",
    "    \n",
    "    b = np.concatenate((np.ones(len(X)-k), np.zeros(k)))[:,None]\n",
    "\n",
    "    X = np.insert(X,[0],b,axis=1)\n",
    "\n",
    "    w_fit=np.linalg.lstsq(X, yy, rcond=None)[0]\n",
    "    \n",
    "    \n",
    "    return w_fit[1:], w_fit[0]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf983918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias is: 0.09105350649797651\n",
      "Weights are: \n",
      " [-6.08615525e-02 -1.06095667e-01  7.53031143e-02  2.78057665e-01\n",
      "  2.59611794e-01  1.19816553e-01  1.05110635e-02  2.32613667e-01\n",
      " -3.17361266e-01 -7.77510933e-02 -4.17318470e-02  4.99788622e-02\n",
      " -2.21837493e-02 -2.47999863e-03 -7.29573201e-03  3.75843616e-02\n",
      "  2.67414563e-02  3.32447940e-03 -8.15353393e-02  1.75498538e-01\n",
      "  7.92614640e-02  2.86048166e-02 -4.09313322e-02 -6.22094524e-03\n",
      " -6.01902872e-02 -5.63195592e-02 -1.47997259e-02 -4.11106517e-01\n",
      "  3.40469070e-01  2.63461215e-01 -1.35315466e-02  1.73477525e-03\n",
      " -9.11983421e-03 -6.98890681e-03  6.51972330e-02 -1.75618988e-01\n",
      " -1.79686338e-01  2.20532723e-01 -9.06442066e-02 -3.28680365e-02\n",
      " -1.80905054e-01  1.91033029e-02 -1.75010207e-01  5.31362120e-02\n",
      " -1.40996396e-01 -2.02005581e-02 -2.04166648e-02  2.13763380e-02\n",
      " -4.53472384e-01  8.70663389e-02 -9.25631074e-02 -3.52428163e-03\n",
      " -7.86791379e-02 -5.89370945e-02 -3.60229412e-02 -8.00712607e-02\n",
      "  4.99929907e-02 -1.32414827e-01  1.16614759e-01 -7.67192217e-02\n",
      "  6.28052858e-02 -1.36156590e-02 -1.24405283e-01 -1.11962320e-01\n",
      " -1.05745595e-01 -1.09096265e-01  1.02784907e-01  1.29260645e-01\n",
      "  1.62385488e-02  8.29137144e-02  1.21588397e-01  2.35635620e-02\n",
      "  6.00428331e-02  5.36065688e-02 -1.58943047e-01 -2.10638769e-01\n",
      "  5.53642172e-02 -4.45653126e-02 -6.35137027e-02  2.30745748e-02\n",
      " -3.49447157e-03 -9.14451236e-02 -1.75856317e-02  1.81592014e-01\n",
      " -1.10382344e-02 -8.85325246e-02  6.53297518e-02 -1.32855425e-01\n",
      "  6.08046976e-02 -4.46882725e-02  6.62615768e-03  3.18639468e-02\n",
      " -1.31828880e-01  3.15130603e-01  4.07034188e-01  2.93057314e-01\n",
      " -8.35626603e-02 -1.86794992e-03  3.90480187e-02  1.23036198e-02\n",
      "  3.61804384e-02  3.92992483e-02  2.22181868e-01  1.64155653e-02\n",
      " -8.38710383e-02 -9.79048545e-02 -1.87848423e-01  3.88340118e-03\n",
      " -3.27803517e-02  6.24800330e-02  2.41386867e-01  1.53100292e-01\n",
      "  1.17882022e-01  5.04847348e-03 -2.78044147e-01 -2.25026918e-01\n",
      " -2.09808590e-01  1.01635052e-01  4.09812789e-02 -5.67527959e-02\n",
      " -7.23040667e-03 -1.30984324e-02 -3.29769036e-02 -1.62741547e-02\n",
      " -1.13666894e-01  4.68472791e-02 -9.78591687e-02  6.50160010e-02\n",
      "  2.24139118e-01 -1.09502517e-01  3.09160555e-02 -1.91953202e-01\n",
      " -9.05907427e-02 -1.42260643e-01 -5.37269731e-02  8.62678637e-02\n",
      " -2.66529808e-02 -6.77824445e-02  6.67732153e-02  1.31781051e-01\n",
      "  2.32616524e-02 -7.07380208e-02 -8.02978017e-02 -5.36813806e-02\n",
      "  1.21452168e-01 -4.12531822e-02 -7.21100203e-02 -9.10778881e-02\n",
      " -9.67135577e-02  6.00292166e-02 -1.88987557e-02  1.31334720e-02\n",
      " -8.05177925e-02 -5.12040306e-03  2.04261416e-02  8.25998913e-02\n",
      " -1.48388899e-02  3.94345952e-02 -1.56402514e-03 -7.55938932e-02\n",
      "  2.00786580e-02 -5.56474326e-02  7.08819738e-02 -1.13715304e-02\n",
      " -3.28839980e-01  1.68364694e-01 -1.17863786e-01 -8.48665832e-03\n",
      " -1.52278253e-02 -2.02244340e-04 -1.04691015e-01  1.23152991e-02\n",
      " -1.51840471e-01 -9.01151212e-02  1.82616179e-01 -1.12551601e-01\n",
      " -2.80053874e-02  1.00012582e-01  5.60763691e-02 -7.07102557e-03\n",
      " -5.71587140e-03 -7.78592943e-02 -1.95761990e-01  1.55958254e-03\n",
      "  1.35107230e-01  1.49494089e-02 -4.16701782e-02  2.88933571e-02\n",
      " -1.08624480e-01 -6.28914888e-02  9.04252121e-03 -3.15405580e-01\n",
      " -5.91950717e-02 -1.71065405e-02 -4.84207895e-02 -1.54606340e-02\n",
      " -1.27313580e-02  2.55973818e-02 -5.12230040e-02  7.79719587e-02\n",
      " -9.59492930e-03 -5.65689090e-02 -4.60056888e-02 -9.92881443e-02\n",
      "  9.34154435e-02  1.38937723e-02  5.55656115e-02 -1.03523060e-01\n",
      " -2.96521999e-03 -1.97373336e-01 -1.15915336e-01  2.45377146e-02\n",
      " -7.51284963e-02  2.95901966e-02  1.59732088e-01 -2.55214932e-02\n",
      " -7.05921972e-02 -1.18534101e-01 -1.28853400e-01 -1.90248379e-01\n",
      " -1.15517031e-01  1.72491018e-01 -1.35081590e-01 -8.82162375e-04\n",
      " -1.75898826e-01  4.37553559e-02 -3.36990974e-02  3.78125873e-02\n",
      " -1.00942340e-01 -1.07945331e-01  1.38563027e-01 -1.08563155e-01\n",
      " -1.77090374e-03  1.44306506e-02 -1.61824091e-01  2.10669305e-02\n",
      " -3.69785996e-02  5.22684779e-02  6.65311216e-01 -7.56237913e-02\n",
      " -2.93882803e-02  4.65750132e-02 -2.07112247e-02  8.17381936e-02\n",
      "  3.18926466e-02 -6.05938774e-03 -2.34145665e-01  2.07264826e-01\n",
      "  2.68748291e-02 -2.75100461e-02  5.75702227e-06  3.39331308e-02\n",
      " -1.02703189e-02 -3.34138144e-02  1.79871085e-01  9.34575330e-03\n",
      " -8.74855308e-02 -1.74237522e-02  6.20731713e-02 -3.16820832e-02\n",
      "  7.91034238e-02  9.30206520e-02 -5.56862037e-02  6.59649028e-02\n",
      " -4.74246784e-02 -2.46496068e-01 -7.55199652e-02 -1.53792392e-02\n",
      " -2.49783440e-02 -1.49770585e-01  1.19841505e-01 -2.03551806e-03\n",
      " -4.89038308e-02 -4.85247748e-02  2.85348752e-02  4.05902677e-02\n",
      " -2.73977734e-03 -5.56445525e-03 -6.31061172e-02  6.18458128e-02\n",
      "  4.28220481e-02  8.96524038e-03  3.33182578e-02  3.09614954e-02\n",
      "  4.99975435e-02  1.72729357e-01  8.62563689e-03  4.96766019e-02\n",
      " -8.92889404e-02  4.42282517e-03  7.11294571e-03  6.54528089e-02\n",
      "  5.17706544e-02 -1.54278426e-01 -2.09825346e-03 -8.85456425e-04\n",
      " -6.48070821e-02  8.69504566e-02  8.66348821e-02 -2.53237348e-02\n",
      " -3.16526561e-02  2.88542919e-01  2.24807405e-01 -2.48728268e-02\n",
      " -4.47132403e-02 -7.82424973e-02  5.09331541e-02  8.25736293e-02\n",
      "  3.41910251e-02  1.02186257e-01  1.43144995e-01 -2.33661646e-02\n",
      "  3.45967092e-02 -3.03613113e-02 -1.39112619e-01  3.05587465e-02\n",
      "  6.83481218e-03 -4.39850740e-02 -1.17374952e-01 -6.91175299e-02\n",
      " -4.47527417e-02 -3.25959504e-02 -3.84450276e-02  2.55009541e-02\n",
      "  1.82002984e-02  6.79272720e-02 -7.68406115e-02 -2.47539196e-02\n",
      " -2.89360719e-02 -3.75559461e-02 -7.37622592e-02  9.46551993e-02\n",
      "  1.57810076e-01 -1.55469957e-01  7.88141999e-02  3.16405169e-02\n",
      "  2.17205686e-02  2.19262730e-02  6.28124114e-02  1.14806498e-01\n",
      "  1.36989292e-01 -6.02040162e-04  6.66557491e-04 -2.80053279e-02\n",
      " -1.18440708e-03 -5.65586184e-03  6.71224631e-02  1.00568659e-01\n",
      " -1.64279314e-01  8.65653643e-03  3.96954653e-02 -1.43297161e-01\n",
      " -4.73488855e-02  6.81528053e-02  1.83899948e-02 -8.73701675e-03\n",
      " -7.72313103e-02 -1.46353459e-02 -1.01985546e-02 -2.68701222e-02\n",
      " -5.40211418e-02  1.14790736e-02 -4.76934694e-02 -2.71385451e-01\n",
      "  1.45368351e-01  1.03736407e-02 -7.82356022e-03 -2.05339687e-02\n",
      "  5.88759483e-02  5.55323179e-02  1.18627774e-01  2.15560819e-01\n",
      "  3.60998227e-02]\n"
     ]
    }
   ],
   "source": [
    "alpha=30\n",
    "\n",
    "ww0, bb0 = fit_linreg(X_train, y_train, alpha)\n",
    "\n",
    "print(\"Bias is:\",bb0)\n",
    "print(\"Weights are:\",\"\\n\",ww0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff525d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Support code\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from scipy.linalg import cho_factor, cho_solve\n",
    "\n",
    "def params_unwrap(param_vec, shapes, sizes):\n",
    "    \"\"\"Helper routine for minimize_list\"\"\"\n",
    "    args = []\n",
    "    pos = 0\n",
    "    for i in range(len(shapes)):\n",
    "        sz = sizes[i]\n",
    "        args.append(param_vec[pos:pos+sz].reshape(shapes[i]))\n",
    "        pos += sz\n",
    "    return args\n",
    "\n",
    "\n",
    "def params_wrap(param_list):\n",
    "    \"\"\"Helper routine for minimize_list\"\"\"\n",
    "    param_list = [np.array(x) for x in param_list]\n",
    "    shapes = [x.shape for x in param_list]\n",
    "    sizes = [x.size for x in param_list]\n",
    "    param_vec = np.zeros(sum(sizes))\n",
    "    pos = 0\n",
    "    for param in param_list:\n",
    "        sz = param.size\n",
    "        param_vec[pos:pos+sz] = param.ravel()\n",
    "        pos += sz\n",
    "    unwrap = lambda pvec: params_unwrap(pvec, shapes, sizes)\n",
    "    return param_vec, unwrap\n",
    "\n",
    "\n",
    "def linreg_cost(params, X, yy, alpha):\n",
    "    \"\"\"Regularized least squares cost function and gradients\n",
    "\n",
    "    Can be optimized with minimize_list -- see fit_linreg_gradopt for a\n",
    "    demonstration.\n",
    "\n",
    "    Inputs:\n",
    "    params: tuple (ww, bb): weights ww (D,), bias bb scalar\n",
    "         X: N,D design matrix of input features\n",
    "        yy: N,  real-valued targets\n",
    "     alpha: regularization constant\n",
    "\n",
    "    Outputs: (E, [ww_bar, bb_bar]), cost and gradients\n",
    "    \"\"\"\n",
    "    # Unpack parameters from list\n",
    "    ww, bb = params\n",
    "\n",
    "    # forward computation of error\n",
    "    ff = np.dot(X, ww) + bb\n",
    "    res = ff - yy\n",
    "    E = np.dot(res, res) + alpha*np.dot(ww, ww)\n",
    "\n",
    "    # reverse computation of gradients\n",
    "    ff_bar = 2*res\n",
    "    bb_bar = np.sum(ff_bar)\n",
    "    ww_bar = np.dot(X.T, ff_bar) + 2*alpha*ww\n",
    "\n",
    "    return E, [ww_bar, bb_bar]\n",
    "\n",
    "def minimize_list(cost, init_list, args):\n",
    "    \"\"\"Optimize a list of arrays (wrapper of scipy.optimize.minimize)\n",
    "\n",
    "    The input function \"cost\" should take a list of parameters,\n",
    "    followed by any extra arguments:\n",
    "        cost(init_list, *args)\n",
    "    should return the cost of the initial condition, and a list in the same\n",
    "    format as init_list giving gradients of the cost wrt the parameters.\n",
    "\n",
    "    The options to the optimizer have been hard-coded. You may wish\n",
    "    to change disp to True to get more diagnostics. You may want to\n",
    "    decrease maxiter while debugging. Although please report all results\n",
    "    in Q2-5 using maxiter=500.\n",
    "    \"\"\"\n",
    "    opt = {'maxiter': 500, 'disp': False}\n",
    "    init, unwrap = params_wrap(init_list)\n",
    "    def wrap_cost(vec, *args):\n",
    "        E, params_bar = cost(unwrap(vec), *args)\n",
    "        vec_bar, _ = params_wrap(params_bar)\n",
    "        return E, vec_bar\n",
    "    res = minimize(wrap_cost, init, args, 'L-BFGS-B', jac=True, options=opt)\n",
    "    return unwrap(res.x)\n",
    "\n",
    "\n",
    "def fit_linreg_gradopt(X, yy, alpha):\n",
    "    \"\"\"\n",
    "    fit a regularized linear regression model with gradient opt\n",
    "\n",
    "         ww, bb = fit_linreg_gradopt(X, yy, alpha)\n",
    "\n",
    "     Find weights and bias by using a gradient-based optimizer\n",
    "     (minimize_list) to improve the regularized least squares cost:\n",
    "\n",
    "       np.sum(((np.dot(X,ww) + bb) - yy)**2) + alpha*np.dot(ww,ww)\n",
    "\n",
    "     Inputs:\n",
    "             X N,D design matrix of input features\n",
    "            yy N,  real-valued targets\n",
    "         alpha     scalar regularization constant\n",
    "\n",
    "     Outputs:\n",
    "            ww D,  fitted weights\n",
    "            bb     scalar fitted bias\n",
    "    \"\"\"\n",
    "    D = X.shape[1]\n",
    "    args = (X, yy, alpha)\n",
    "    init = (np.zeros(D), np.array(0))\n",
    "    ww, bb = minimize_list(linreg_cost, init, args)\n",
    "    return ww, bb\n",
    "\n",
    "def logreg_cost(params, X, yy, alpha):\n",
    "    \"\"\"Regularized logistic regression cost function and gradients\n",
    "\n",
    "    Can be optimized with minimize_list -- see fit_linreg_gradopt for a\n",
    "    demonstration of fitting a similar function.\n",
    "\n",
    "    Inputs:\n",
    "    params: tuple (ww, bb): weights ww (D,), bias bb scalar\n",
    "         X: N,D design matrix of input features\n",
    "        yy: N,  real-valued targets\n",
    "     alpha: regularization constant\n",
    "\n",
    "    Outputs: (E, [ww_bar, bb_bar]), cost and gradients\n",
    "    \"\"\"\n",
    "    # Unpack parameters from list\n",
    "    ww, bb = params\n",
    "\n",
    "    # Force targets to be +/- 1\n",
    "    yy = 2*(yy==1) - 1\n",
    "\n",
    "    # forward computation of error\n",
    "    aa = yy*(np.dot(X, ww) + bb)\n",
    "    sigma = 1/(1 + np.exp(-aa))\n",
    "    E = -np.sum(np.log(sigma)) + alpha*np.dot(ww, ww)\n",
    "\n",
    "    # reverse computation of gradients\n",
    "    aa_bar = sigma - 1\n",
    "    bb_bar = np.dot(aa_bar, yy)\n",
    "    ww_bar = np.dot(X.T, yy*aa_bar) + 2*alpha*ww\n",
    "\n",
    "    return E, (ww_bar, bb_bar)\n",
    "\n",
    "def nn_cost(params, X, yy=None, alpha=None):\n",
    "    \"\"\"NN_COST simple neural network cost function and gradients, or predictions\n",
    "\n",
    "           E, params_bar = nn_cost([ww, bb, V, bk], X, yy, alpha)\n",
    "                    pred = nn_cost([ww, bb, V, bk], X)\n",
    "\n",
    "     Cost function E can be minimized with minimize_list\n",
    "\n",
    "     Inputs:\n",
    "             params (ww, bb, V, bk), where:\n",
    "                    --------------------------------\n",
    "                        ww K,  hidden-output weights\n",
    "                        bb     scalar output bias\n",
    "                         V K,D hidden-input weights\n",
    "                        bk K,  hidden biases\n",
    "                    --------------------------------\n",
    "                  X N,D input design matrix\n",
    "                 yy N,  regression targets\n",
    "              alpha     scalar regularization for weights\n",
    "\n",
    "     Outputs:\n",
    "                     E  sum of squares error\n",
    "            params_bar  gradients wrt params, same format as params\n",
    "     OR\n",
    "               pred N,  predictions if only params and X are given as inputs\n",
    "    \"\"\"\n",
    "    # Unpack parameters from list\n",
    "    ww, bb, V, bk = params\n",
    "\n",
    "    # Forwards computation of cost\n",
    "    A = np.dot(X, V.T) + bk[None,:] # N,K\n",
    "    P = 1 / (1 + np.exp(-A)) # N,K\n",
    "    F = np.dot(P, ww) + bb # N,\n",
    "    if yy is None:\n",
    "        # user wants prediction rather than training signal:\n",
    "        return F\n",
    "    res = F - yy # N,\n",
    "    E = np.dot(res, res) + alpha*(np.sum(V*V) + np.dot(ww,ww)) # 1x1\n",
    "\n",
    "    # Reverse computation of gradients\n",
    "    F_bar = 2*res # N,\n",
    "    ww_bar = np.dot(P.T, F_bar) + 2*alpha*ww # K,\n",
    "    bb_bar = np.sum(F_bar) # scalar\n",
    "    P_bar = np.dot(F_bar[:,None], ww[None,:]) # N,K\n",
    "    A_bar = P_bar * P * (1 - P) # N,K\n",
    "    V_bar = np.dot(A_bar.T, X) + 2*alpha*V # K,D\n",
    "    bk_bar = np.sum(A_bar, 0)\n",
    "\n",
    "    return E, (ww_bar, bb_bar, V_bar, bk_bar)\n",
    "\n",
    "def rbf_fn(X1, X2):\n",
    "    \"\"\"Helper routine for gp_post_par\"\"\"\n",
    "    return np.exp((np.dot(X1,(2*X2.T))-np.sum(X1*X1,1)[:,None]) - np.sum(X2*X2,1)[None,:])\n",
    "\n",
    "\n",
    "def gauss_kernel_fn(X1, X2, ell, sigma_f):\n",
    "    \"\"\"Helper routine for gp_post_par\"\"\"\n",
    "    return sigma_f**2 * rbf_fn(X1/(np.sqrt(2)*ell), X2/(np.sqrt(2)*ell))\n",
    "\n",
    "\n",
    "def gp_post_par(X_rest, X_obs, yy, sigma_y=0.05, ell=5.0, sigma_f=0.1):\n",
    "    \"\"\"GP_POST_PAR means and covariances of a posterior Gaussian process\n",
    "\n",
    "         rest_cond_mu, rest_cond_cov = gp_post_par(X_rest, X_obs, yy)\n",
    "         rest_cond_mu, rest_cond_cov = gp_post_par(X_rest, X_obs, yy, sigma_y, ell, sigma_f)\n",
    "\n",
    "     Calculate the means and covariances at all test locations of the posterior Gaussian\n",
    "     process conditioned on the observations yy at observed locations X_obs.\n",
    "\n",
    "     Inputs:\n",
    "                 X_rest GP test locations\n",
    "                  X_obs locations of observations\n",
    "                     yy observed values\n",
    "                sigma_y observation noise standard deviation\n",
    "                    ell kernel function length scale\n",
    "                sigma_f kernel function standard deviation\n",
    "\n",
    "     Outputs:\n",
    "           rest_cond_mu mean at each location in X_rest\n",
    "          rest_cond_cov covariance matrix between function values at all test locations\n",
    "    \"\"\"\n",
    "    X_rest = X_rest[:, None]\n",
    "    X_obs = X_obs[:, None]\n",
    "    K_rest = gauss_kernel_fn(X_rest, X_rest, ell, sigma_f)\n",
    "    K_rest_obs = gauss_kernel_fn(X_rest, X_obs, ell, sigma_f)\n",
    "    K_obs = gauss_kernel_fn(X_obs, X_obs, ell, sigma_f)\n",
    "    M = K_obs + sigma_y**2 * np.eye(yy.size)\n",
    "    M_cho, M_low = cho_factor(M)\n",
    "    rest_cond_mu = np.dot(K_rest_obs, cho_solve((M_cho, M_low), yy))\n",
    "    rest_cond_cov = K_rest - np.dot(K_rest_obs, cho_solve((M_cho, M_low), K_rest_obs.T))\n",
    "\n",
    "    return rest_cond_mu, rest_cond_cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1262a835",
   "metadata": {},
   "outputs": [],
   "source": [
    "ww1,bb1 = fit_linreg_gradopt(X_train, y_train, 30)\n",
    "#print(bb1,\"\\n\")\n",
    "#print(ww1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc55291c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(pred,yy):\n",
    "    return np.sqrt(np.mean((pred-yy)**2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99418f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root means square for training set(using least square method): 0.3567565397204054\n",
      "Root means square for validation set(using least square method): 0.4230521968394701\n"
     ]
    }
   ],
   "source": [
    "## for least square method\n",
    "pred1_train = np.dot(X_train,ww0)+bb0\n",
    "pred2_val = np.dot(X_val,ww0)+bb0\n",
    "print(\"Root means square for training set(using least square method):\",rmse(pred1_train, y_train))\n",
    "print(\"Root means square for validation set(using least square method):\",rmse(pred2_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b126c2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root means square for training set(using least square method): 0.3567569385517838\n",
      "Root means square for validation set(using least square method): 0.4230540100048547\n"
     ]
    }
   ],
   "source": [
    "# for gradient method\n",
    "pred1_train = np.dot(X_train,ww1)+bb1\n",
    "pred2_val = np.dot(X_val,ww1)+bb1\n",
    "print(\"Root means square for training set(using least square method):\",rmse(pred1_train, y_train))\n",
    "print(\"Root means square for validation set(using least square method):\",rmse(pred2_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab04f282",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################  3rd question #############################################################\n",
    "\n",
    "def fit_logreg_gradopt(X, yy, alpha):\n",
    "    \"\"\"\n",
    "    fit a regularized linear regression model with gradient opt\n",
    "\n",
    "         ww, bb = fit_linreg_gradopt(X, yy, alpha)\n",
    "\n",
    "     Find weights and bias by using a gradient-based optimizer\n",
    "     (minimize_list) to improve the regularized least squares cost:\n",
    "\n",
    "       np.sum(((np.dot(X,ww) + bb) - yy)**2) + alpha*np.dot(ww,ww)\n",
    "\n",
    "     Inputs:\n",
    "             X N,D design matrix of input features\n",
    "            yy N,  real-valued targets\n",
    "         alpha     scalar regularization constant\n",
    "\n",
    "     Outputs:\n",
    "            ww D,  fitted weights\n",
    "            bb     scalar fitted bias\n",
    "    \"\"\"\n",
    "    D = X.shape[1]\n",
    "    args = (X, yy, alpha)\n",
    "    init = (np.zeros(D), np.array(0))\n",
    "    ww, bb = minimize_list(logreg_cost, init, args)\n",
    "    return ww, bb\n",
    "\n",
    "\n",
    "K = 20 # number of thresholded classification problems to fit\n",
    "mx = np.max(y_train); mn = np.min(y_train); hh = (mx-mn)/(K+1)\n",
    "thresholds = np.linspace(mn+hh, mx-hh, num=K, endpoint=True)\n",
    "\n",
    "w_fit2= np.array([[0.0]* (len(X_train[1])+1)] * K)\n",
    "for kk in range(K):\n",
    "    labels = y_train > thresholds[kk]\n",
    "    ww2, bb2 = fit_logreg_gradopt(X_train, labels, alpha=30)\n",
    "    w_fit2[kk,0] = bb2\n",
    "    w_fit2[kk,1:]=ww2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed9bef1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e1ca273",
   "metadata": {},
   "outputs": [],
   "source": [
    "bb2_hat = w_fit2[:,0]\n",
    "ww2_hat = w_fit2[:,1:]\n",
    "\n",
    "def sigmoid(a):\n",
    "    return 1 / (1+np.exp(-a))\n",
    "\n",
    "\n",
    "\n",
    "X_train_new = sigmoid(np.dot(X_train, np.transpose(ww2_hat))+bb2_hat) \n",
    "X_val_new = sigmoid(np.dot(X_val, np.transpose(ww2_hat))+bb2_hat)\n",
    "\n",
    "\n",
    "nn_ww, nn_bb = fit_linreg(X_train_new, y_train, alpha=30)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "546bbdc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root means square for training set: 0.15441150430439968\n",
      "Root means square for validation set: 0.2542477298370707\n"
     ]
    }
   ],
   "source": [
    "pred1_train = np.dot(X_train_new, nn_ww) + nn_bb\n",
    "pred2_val = np.dot(X_val_new, nn_ww) + nn_bb\n",
    "\n",
    "print(\"Root means square for training set:\",rmse(pred1_train, y_train))\n",
    "print(\"Root means square for validation set:\",rmse(pred2_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0e8f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72bafe6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = len(X_train[1]) \n",
    "mod_ww = np.random.randn(l,l-1) * np.sqrt(2/(l-1))\n",
    "\n",
    "\n",
    "################################# Question 4 #####################################################################\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "def fit_nn_gradopt(X, yy, K, alpha, w_random = True):\n",
    "    \"\"\"\n",
    "    fit a regularized linear regression model with gradient opt\n",
    "\n",
    "         ww, bb = fit_linreg_gradopt(X, yy, alpha)\n",
    "\n",
    "     Find weights and bias by using a gradient-based optimizer\n",
    "     (minimize_list) to improve the regularized least squares cost:\n",
    "\n",
    "       np.sum(((np.dot(X,ww) + bb) - yy)**2) + alpha*np.dot(ww,ww)\n",
    "\n",
    "     Inputs:\n",
    "             X N,D design matrix of input features\n",
    "            yy N,  real-valued targets\n",
    "         alpha     scalar regularization constant\n",
    "\n",
    "     Outputs:\n",
    "            ww D,  fitted weights\n",
    "            bb     scalar fitted bias\n",
    "    \"\"\"\n",
    "    args = (X, yy, alpha)\n",
    "    \n",
    "    if w_random:\n",
    "        D = len(X_train[1])\n",
    "        # calculate the range for the weights\n",
    "        l = len(X_train[1]) \n",
    "    \n",
    "        # generate random numbers\n",
    "        ww = 0.1 * np.random.randn(K) / np.sqrt(K)\n",
    "        V = 0.1 * np.random.randn(K,D)/ np.sqrt(D)\n",
    "        bk = np.zeros(K)\n",
    "        bb = 0\n",
    "        init = (ww, bb, V, bk)\n",
    "        ww, bb, V, bk = minimize_list(nn_cost, init, args)\n",
    "        return (ww, bb, V, bk)\n",
    "    \n",
    "    else:\n",
    "        init = (nn_ww,nn_bb,ww2_hat, bb2_hat)            ## Initialization from the results we obtained \n",
    "        ww, bb, V, bk = minimize_list(nn_cost, init, args)\n",
    "        return (ww, bb, V, bk)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "14b003ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set RMSE for NN(with random initialization): 0.14034036181646106\n",
      "Validation set RMSE for NN(with random initialization): 0.2701938418376688\n"
     ]
    }
   ],
   "source": [
    "params = fit_nn_gradopt(X_train, y_train,K=20, alpha=30)\n",
    "pred_train_nn= nn_cost(params, X_train, yy=None, alpha=30)\n",
    "pred_val_nn= nn_cost(params, X_val, yy=None, alpha=30)\n",
    "print(\"Training set RMSE for NN(with random initialization):\",rmse(pred_train_nn, y_train))\n",
    "print(\"Validation set RMSE for NN(with random initialization):\",rmse(pred_val_nn, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "608d25ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set RMSE for NN: 0.13972707083230795\n",
      "Validation set RMSE for NN: 0.26789560258337225\n"
     ]
    }
   ],
   "source": [
    "params2 = fit_nn_gradopt(X_train, y_train,K=20, alpha=30, w_random = False)\n",
    "\n",
    "pred1_train_nn = nn_cost(params2, X_train, yy=None, alpha=30)\n",
    "pred1_val_nn = nn_cost(params2, X_val, yy=None, alpha=30)\n",
    "\n",
    "print(\"Training set RMSE for NN:\",rmse(pred1_train_nn, y_train))\n",
    "print(\"Validation set RMSE for NN:\",rmse(pred1_val_nn, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9778ac0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation RMSE for the value of alpha 40.32 is 0.2812710293792219\n",
      "Validation RMSE for the value of alpha 25.060000000000002 is 0.27946616218715065\n",
      "Validation RMSE for the value of alpha 34.86 is 0.27610400343109814\n"
     ]
    }
   ],
   "source": [
    "\n",
    "######################################## 5 question ###############################################################\n",
    "\n",
    "def train_nn_reg(X_train, X_val, yy, y_val, train_alpha):\n",
    "    \n",
    "    param = fit_nn_gradopt(X_train, yy, K=20, alpha= train_alpha)\n",
    "    \n",
    "    pred_val = nn_cost(param, X_val, yy=None, alpha= train_alpha)\n",
    "\n",
    "    return (rmse(pred_val,y_val), param)\n",
    "    \n",
    "    \n",
    "alpha= np.arange(0,50,0.02)\n",
    "\n",
    "indicies = np.random.choice(len(alpha),3) \n",
    "obs_alpha = np.array(alpha[indicies])\n",
    "test_alpha = np.delete(alpha,indicies)\n",
    "\n",
    "obs_alpha_val = np.array([])\n",
    "\n",
    "for alpha in obs_alpha: \n",
    "    val_rmse = train_nn_reg(X_train, X_val, y_train, y_val, alpha)[0]\n",
    "    obs_alpha_val = np.append(obs_alpha_val, val_rmse )\n",
    "    print(\"Validation RMSE for the value of alpha {0} is {1}\".format(alpha,\n",
    "       val_rmse))\n",
    "          \n",
    "    \n",
    "\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "60e9900b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum probability for Alpha(=0.0) is 0.7475321023653295 and Validation RMSE is: 0.2678971604781454\n",
      "Maximum probability for Alpha(=11.620000000000001) is 0.629239125108158 and Validation RMSE is: 0.24686535718421992\n",
      "Maximum probability for Alpha(=11.74) is 0.41327583107173793 and Validation RMSE is: 0.2599489909924994\n",
      "Maximum probability for Alpha(=49.980000000000004) is 0.29053708207437645 and Validation RMSE is: 0.2833741246075129\n",
      "Maximum probability for Alpha(=7.94) is 0.28881691451709535 and Validation RMSE is: 0.23827218248951826\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats\n",
    "\n",
    "\n",
    "\n",
    "log_base_rmse = np.log(0.25829517688803216)    ## Validation rmse from question 4 (for Best model)\n",
    "\n",
    "y = np.array(log_base_rmse - np.log(obs_alpha_val))\n",
    "\n",
    "post_mean, post_cov  = gp_post_par(test_alpha, obs_alpha, y)\n",
    " \n",
    "post_std = np.sqrt(np.diag(post_cov))         ## Standard deviation\n",
    "\n",
    "def phi(post_mean, post_std, y):\n",
    "    return scipy.stats.norm.cdf((post_mean - max(y))/post_std)\n",
    "\n",
    "best_alpha = 0.0\n",
    "best_alpha_rmse = 9999.0\n",
    "best_params = set()\n",
    "for _ in range(5):\n",
    "    prob_max = phi(post_mean, post_std, y)\n",
    "    idx = np.argmax(prob_max)\n",
    "    \n",
    "    \n",
    "    alpha_val_rmse, params = train_nn_reg(X_train, X_val, y_train, y_val, test_alpha[idx])\n",
    "    \n",
    "    if  alpha_val_rmse < best_alpha_rmse:\n",
    "        best_alpha = test_alpha[idx]\n",
    "        best_alpha_rmse = alpha_val_rmse\n",
    "        best_params = params\n",
    "    \n",
    "    print(\"Maximum probability for Alpha(={0}) is {1} and Validation RMSE is: {2}\".format( \n",
    "                                test_alpha[idx], prob_max[idx], alpha_val_rmse))\n",
    "    \n",
    "    obs_alpha_val = np.append(obs_alpha_val, alpha_val_rmse)\n",
    "    \n",
    "    obs_alpha = np.append(obs_alpha, test_alpha[idx])\n",
    "    test_alpha = np.delete(test_alpha,idx)\n",
    "\n",
    "    y = np.array(log_base_rmse - np.log(obs_alpha_val))\n",
    "    post_mean, post_cov  = gp_post_par(test_alpha, obs_alpha, y)\n",
    "    post_std = np.sqrt(np.diag(post_cov))\n",
    "   \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce61cd80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Best value for alpha is 7.94\n",
      "The Validation error is 0.23827218248951826\n",
      "The Test error is 0.27217754983889664\n"
     ]
    }
   ],
   "source": [
    "## Traning on best alpha to get test error\n",
    "pred_test = nn_cost(best_params, X_test, yy=None, alpha=best_alpha)   ## Prediction for test set\n",
    "test_error = rmse(pred_test, y_test)\n",
    "\n",
    "print(\"The Best value for alpha is {0}\".format(best_alpha))\n",
    "print(\"The Validation error is {0}\".format(best_alpha_rmse))\n",
    "print(\"The Test error is {0}\".format(test_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d9b999b",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################### Question 6 ###############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "46a8c81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_logreg_gradopt(X, yy, alpha):\n",
    "    \"\"\"\n",
    "    fit a regularized linear regression model with gradient opt\n",
    "\n",
    "         ww, bb = fit_linreg_gradopt(X, yy, alpha)\n",
    "\n",
    "     Find weights and bias by using a gradient-based optimizer\n",
    "     (minimize_list) to improve the regularized least squares cost:\n",
    "\n",
    "       np.sum(((np.dot(X,ww) + bb) - yy)**2) + alpha*np.dot(ww,ww)\n",
    "\n",
    "     Inputs:\n",
    "             X N,D design matrix of input features\n",
    "            yy N,  real-valued targets\n",
    "         alpha     scalar regularization constant\n",
    "\n",
    "     Outputs:\n",
    "            ww D,  fitted weights\n",
    "            bb     scalar fitted bias\n",
    "    \"\"\"\n",
    "    D = X.shape[1]\n",
    "    args = (X, yy, alpha)\n",
    "    init = (np.zeros(D), np.array(0))\n",
    "    ww, bb = minimize_list(logreg_cost, init, args)\n",
    "    return ww, bb\n",
    "\n",
    "\n",
    "K = 26 # number of thresholded classification problems to fit\n",
    "mx = np.max(y_train); mn = np.min(y_train); hh = (mx-mn)/(K+1)\n",
    "thresholds = np.linspace(mn+hh, mx-hh, num=K, endpoint=True)\n",
    "\n",
    "ww_fit2= np.array([[0.0]* (len(X_train[1])+1)] * K)\n",
    "for kk in range(K):\n",
    "    labels = y_train > thresholds[kk]\n",
    "    ww2, bb2 = fit_logreg_gradopt(X_train, labels, alpha=best_alpha)\n",
    "    ww_fit2[kk,0] = bb2\n",
    "    ww_fit2[kk,1:]=ww2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "66807443",
   "metadata": {},
   "outputs": [],
   "source": [
    "bb3_hat = ww_fit2[:,0]\n",
    "ww3_hat = ww_fit2[:,1:]\n",
    "\n",
    "def relu(a):\n",
    "    return np.maximum(0.0, a)\n",
    "## Derivative of relu\n",
    "def deriv_relu(a):\n",
    "    return a > 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_train_new2 = sigmoid(np.dot(X_train, np.transpose(ww3_hat))+bb3_hat) \n",
    "X_val_new2 = sigmoid(np.dot(X_val, np.transpose(ww3_hat))+bb3_hat)\n",
    "\n",
    "\n",
    "nn_ww2, nn_bb2 = fit_linreg(X_train_new2, y_train, alpha=best_alpha)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b9351846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root means square for training set: 0.11401970484862825\n",
      "Root means square for validation set: 0.2517460515316447\n"
     ]
    }
   ],
   "source": [
    "pred1_train = np.dot(X_train_new2, nn_ww2) + nn_bb2\n",
    "pred2_val = np.dot(X_val_new2, nn_ww2) + nn_bb2\n",
    "\n",
    "print(\"Root means square for training set:\",rmse(pred1_train, y_train))\n",
    "print(\"Root means square for validation set:\",rmse(pred2_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5c71c633",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_cost(params, X, yy=None, alpha=None):\n",
    "    \"\"\"NN_COST simple neural network cost function and gradients, or predictions\n",
    "\n",
    "           E, params_bar = nn_cost([ww, bb, V, bk], X, yy, alpha)\n",
    "                    pred = nn_cost([ww, bb, V, bk], X)\n",
    "\n",
    "     Cost function E can be minimized with minimize_list\n",
    "\n",
    "     Inputs:\n",
    "             params (ww, bb, V, bk), where:\n",
    "                    --------------------------------\n",
    "                        ww K,  hidden-output weights\n",
    "                        bb     scalar output bias\n",
    "                         V K,D hidden-input weights\n",
    "                        bk K,  hidden biases\n",
    "                    --------------------------------\n",
    "                  X N,D input design matrix\n",
    "                 yy N,  regression targets\n",
    "              alpha     scalar regularization for weights\n",
    "\n",
    "     Outputs:\n",
    "                     E  sum of squares error\n",
    "            params_bar  gradients wrt params, same format as params\n",
    "     OR\n",
    "               pred N,  predictions if only params and X are given as inputs\n",
    "    \"\"\"\n",
    "    # Unpack parameters from list\n",
    "    ww, bb, V, bk = params\n",
    "\n",
    "    # Forwards computation of cost\n",
    "    A = np.dot(X, V.T) + bk[None,:] # N,K\n",
    "    P = relu(A) # N,K\n",
    "    F = np.dot(P, ww) + bb # N,\n",
    "    if yy is None:\n",
    "        # user wants prediction rather than training signal:\n",
    "        return F\n",
    "    res = F - yy # N,\n",
    "    E = np.dot(res, res) + alpha*(np.sum(V*V) + np.dot(ww,ww)) # 1x1\n",
    "     \n",
    "    # Reverse computation of gradients\n",
    "    F_bar = 2*res # N,\n",
    "    ww_bar = np.dot(P.T, F_bar) + 2*alpha*ww # K,\n",
    "    bb_bar = np.sum(F_bar) # scalar\n",
    "    P_bar = np.dot(F_bar[:,None], ww[None,:]) # N,K\n",
    "    A_bar = P_bar * deriv_relu(P)  # N,K\n",
    "    V_bar = np.dot(A_bar.T, X) + 2*alpha*V # K,D\n",
    "    bk_bar = np.sum(A_bar, 0)\n",
    "\n",
    "    return E, (ww_bar, bb_bar, V_bar, bk_bar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a2c3b8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#l = len(X_train[1]) \n",
    "#mod_ww = np.random.randn(l,l-1) * np.sqrt(2/(l-1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def fit_nn_gradopt(X, yy, alpha):\n",
    "    \"\"\"\n",
    "    fit a regularized linear regression model with gradient opt\n",
    "\n",
    "         ww, bb = fit_linreg_gradopt(X, yy, alpha)\n",
    "\n",
    "     Find weights and bias by using a gradient-based optimizer\n",
    "     (minimize_list) to improve the regularized least squares cost:\n",
    "\n",
    "       np.sum(((np.dot(X,ww) + bb) - yy)**2) + alpha*np.dot(ww,ww)\n",
    "\n",
    "     Inputs:\n",
    "             X N,D design matrix of input features\n",
    "            yy N,  real-valued targets\n",
    "         alpha     scalar regularization constant\n",
    "\n",
    "     Outputs:\n",
    "            ww D,  fitted weights\n",
    "            bb     scalar fitted bias\n",
    "    \"\"\"\n",
    "    args = (X, yy, alpha)\n",
    "    init = (nn_ww2,nn_bb2,ww3_hat, bb3_hat)            ## Initialization from the results we obtained \n",
    "    ww, bb, V, bk = minimize_list(nn_cost, init, args)\n",
    "    return (ww, bb, V, bk)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f4f64a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "params3 = fit_nn_gradopt(X_train, y_train, alpha=best_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b2b7f5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred2_train_nn= nn_cost(params3, X_train, yy=None, alpha=best_alpha)\n",
    "pred2_val_nn= nn_cost(params3, X_val, yy=None, alpha=best_alpha)\n",
    "pred2_test_nn= nn_cost(params3, X_test, yy=None, alpha=best_alpha)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "244bd196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set RMSE for NN: 0.20704405965815648\n",
      "Validation set RMSE for NN: 0.2603849597671114\n",
      "Test set RMSE for NN: 0.3596360240611244\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set RMSE for NN:\",rmse(pred2_train_nn, y_train))\n",
    "print(\"Validation set RMSE for NN:\",rmse(pred2_val_nn, y_val))\n",
    "print(\"Test set RMSE for NN:\",rmse(pred2_test_nn, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3705f01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
