{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e953564f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Assessment 2 MLPR\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "data = np.load('ct_data.npz')\n",
    "X_train = data['X_train']; X_val = data['X_val']; X_test = data['X_test']\n",
    "y_train = data['y_train']; y_val = data['y_val']; y_test = data['y_test']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41853e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of y_train: -9.13868774539957e-15\n",
      "Standard error for y_train (for 5785 entries): 0.011927303389170828\n",
      "Mean of y_val (for 5785 entries): -0.2160085093241599\n",
      "Standard error for y_val (for 5785 entries): 0.01290449880016868\n",
      "                                          \n",
      "Removed column indices are: [69, 351, 199, 359, 78, 79, 179, 188, 189, 287, 59, 69, 179, 189, 351]\n",
      "Modified shape of X_train: (40754, 373)\n",
      "Modified shape of X_val: (5785, 373)\n",
      "Modified shape of X_test: (6961, 373)\n"
     ]
    }
   ],
   "source": [
    "########################## 1st question ####################################### \n",
    "print(\"Mean of y_train:\",np.mean(y_train))\n",
    "print(\"Standard error for y_train (for 5785 entries):\",np.std(y_train[:5785], ddof=1)/np.sqrt(len(y_train[:5785])))\n",
    "print(\"Mean of y_val (for 5785 entries):\",np.mean(y_val))\n",
    "print(\"Standard error for y_val (for 5785 entries):\",np.std(y_val, ddof=1)/np.sqrt(len(y_val)))\n",
    "\n",
    "\n",
    "## Constants\n",
    "\n",
    "rm_idx0=[]    \n",
    "\n",
    "for i in range(len(X_train[1])):\n",
    "    col=X_train[:,i]\n",
    "    if all(col[0]==col):\n",
    "        rm_idx0.append(i)\n",
    "        \n",
    "## Duplicates\n",
    "\n",
    "indices= np.unique(X_train,return_index=True,axis=1)[1]\n",
    "\n",
    "indices=np.sort(indices)\n",
    "rm_idx=list(set(range(indices[0],indices[-1]+1))-set(indices))\n",
    "\n",
    "idx=rm_idx+rm_idx0\n",
    "print(\"                                          \")\n",
    "print(\"Removed column indices are:\",idx)\n",
    "\n",
    "\n",
    "X_train = np.delete(X_train,idx,axis=1)\n",
    "print(\"Modified shape of X_train:\",X_train.shape)\n",
    "\n",
    "X_val = np.delete(X_val, idx, axis=1)\n",
    "print(\"Modified shape of X_val:\",X_val.shape)\n",
    "\n",
    "X_test = np.delete(X_test, idx, axis=1)\n",
    "print(\"Modified shape of X_test:\",X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c18ee684",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2nd question\n",
    "\n",
    "\n",
    "\n",
    "def fit_linreg(X, yy, alpha):\n",
    "    k=len(X[1])                                 ## getting number of input features\n",
    "    yy = np.concatenate((yy, np.zeros(k)))      ## adding 0_k to the y_train array\n",
    "    z_k = np.sqrt(alpha) * np.eye(k)\n",
    "    print(z_k.shape)\n",
    "    X = np.vstack((X,z_k))  \n",
    "    \n",
    "    b = np.concatenate((np.ones(len(X)-k), np.zeros(k)))[:,None]\n",
    "\n",
    "    X = np.insert(X,[0],b,axis=1)\n",
    "\n",
    "    w_fit=np.linalg.lstsq(X, yy, rcond=None)[0]\n",
    "    \n",
    "    \n",
    "    return w_fit[1:], w_fit[0]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf983918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(373, 373)\n",
      "Bias is: 0.09105350649797651\n",
      "Weights are: \n",
      " [-6.08615525e-02 -1.06095667e-01  7.53031143e-02  2.78057665e-01\n",
      "  2.59611794e-01  1.19816553e-01  1.05110635e-02  2.32613667e-01\n",
      " -3.17361266e-01 -7.77510933e-02 -4.17318470e-02  4.99788622e-02\n",
      " -2.21837493e-02 -2.47999863e-03 -7.29573201e-03  3.75843616e-02\n",
      "  2.67414563e-02  3.32447940e-03 -8.15353393e-02  1.75498538e-01\n",
      "  7.92614640e-02  2.86048166e-02 -4.09313322e-02 -6.22094524e-03\n",
      " -6.01902872e-02 -5.63195592e-02 -1.47997259e-02 -4.11106517e-01\n",
      "  3.40469070e-01  2.63461215e-01 -1.35315466e-02  1.73477525e-03\n",
      " -9.11983421e-03 -6.98890681e-03  6.51972330e-02 -1.75618988e-01\n",
      " -1.79686338e-01  2.20532723e-01 -9.06442066e-02 -3.28680365e-02\n",
      " -1.80905054e-01  1.91033029e-02 -1.75010207e-01  5.31362120e-02\n",
      " -1.40996396e-01 -2.02005581e-02 -2.04166648e-02  2.13763380e-02\n",
      " -4.53472384e-01  8.70663389e-02 -9.25631074e-02 -3.52428163e-03\n",
      " -7.86791379e-02 -5.89370945e-02 -3.60229412e-02 -8.00712607e-02\n",
      "  4.99929907e-02 -1.32414827e-01  1.16614759e-01 -7.67192217e-02\n",
      "  6.28052858e-02 -1.36156590e-02 -1.24405283e-01 -1.11962320e-01\n",
      " -1.05745595e-01 -1.09096265e-01  1.02784907e-01  1.29260645e-01\n",
      "  1.62385488e-02  8.29137144e-02  1.21588397e-01  2.35635620e-02\n",
      "  6.00428331e-02  5.36065688e-02 -1.58943047e-01 -2.10638769e-01\n",
      "  5.53642172e-02 -4.45653126e-02 -6.35137027e-02  2.30745748e-02\n",
      " -3.49447157e-03 -9.14451236e-02 -1.75856317e-02  1.81592014e-01\n",
      " -1.10382344e-02 -8.85325246e-02  6.53297518e-02 -1.32855425e-01\n",
      "  6.08046976e-02 -4.46882725e-02  6.62615768e-03  3.18639468e-02\n",
      " -1.31828880e-01  3.15130603e-01  4.07034188e-01  2.93057314e-01\n",
      " -8.35626603e-02 -1.86794992e-03  3.90480187e-02  1.23036198e-02\n",
      "  3.61804384e-02  3.92992483e-02  2.22181868e-01  1.64155653e-02\n",
      " -8.38710383e-02 -9.79048545e-02 -1.87848423e-01  3.88340118e-03\n",
      " -3.27803517e-02  6.24800330e-02  2.41386867e-01  1.53100292e-01\n",
      "  1.17882022e-01  5.04847348e-03 -2.78044147e-01 -2.25026918e-01\n",
      " -2.09808590e-01  1.01635052e-01  4.09812789e-02 -5.67527959e-02\n",
      " -7.23040667e-03 -1.30984324e-02 -3.29769036e-02 -1.62741547e-02\n",
      " -1.13666894e-01  4.68472791e-02 -9.78591687e-02  6.50160010e-02\n",
      "  2.24139118e-01 -1.09502517e-01  3.09160555e-02 -1.91953202e-01\n",
      " -9.05907427e-02 -1.42260643e-01 -5.37269731e-02  8.62678637e-02\n",
      " -2.66529808e-02 -6.77824445e-02  6.67732153e-02  1.31781051e-01\n",
      "  2.32616524e-02 -7.07380208e-02 -8.02978017e-02 -5.36813806e-02\n",
      "  1.21452168e-01 -4.12531822e-02 -7.21100203e-02 -9.10778881e-02\n",
      " -9.67135577e-02  6.00292166e-02 -1.88987557e-02  1.31334720e-02\n",
      " -8.05177925e-02 -5.12040306e-03  2.04261416e-02  8.25998913e-02\n",
      " -1.48388899e-02  3.94345952e-02 -1.56402514e-03 -7.55938932e-02\n",
      "  2.00786580e-02 -5.56474326e-02  7.08819738e-02 -1.13715304e-02\n",
      " -3.28839980e-01  1.68364694e-01 -1.17863786e-01 -8.48665832e-03\n",
      " -1.52278253e-02 -2.02244340e-04 -1.04691015e-01  1.23152991e-02\n",
      " -1.51840471e-01 -9.01151212e-02  1.82616179e-01 -1.12551601e-01\n",
      " -2.80053874e-02  1.00012582e-01  5.60763691e-02 -7.07102557e-03\n",
      " -5.71587140e-03 -7.78592943e-02 -1.95761990e-01  1.55958254e-03\n",
      "  1.35107230e-01  1.49494089e-02 -4.16701782e-02  2.88933571e-02\n",
      " -1.08624480e-01 -6.28914888e-02  9.04252121e-03 -3.15405580e-01\n",
      " -5.91950717e-02 -1.71065405e-02 -4.84207895e-02 -1.54606340e-02\n",
      " -1.27313580e-02  2.55973818e-02 -5.12230040e-02  7.79719587e-02\n",
      " -9.59492930e-03 -5.65689090e-02 -4.60056888e-02 -9.92881443e-02\n",
      "  9.34154435e-02  1.38937723e-02  5.55656115e-02 -1.03523060e-01\n",
      " -2.96521999e-03 -1.97373336e-01 -1.15915336e-01  2.45377146e-02\n",
      " -7.51284963e-02  2.95901966e-02  1.59732088e-01 -2.55214932e-02\n",
      " -7.05921972e-02 -1.18534101e-01 -1.28853400e-01 -1.90248379e-01\n",
      " -1.15517031e-01  1.72491018e-01 -1.35081590e-01 -8.82162375e-04\n",
      " -1.75898826e-01  4.37553559e-02 -3.36990974e-02  3.78125873e-02\n",
      " -1.00942340e-01 -1.07945331e-01  1.38563027e-01 -1.08563155e-01\n",
      " -1.77090374e-03  1.44306506e-02 -1.61824091e-01  2.10669305e-02\n",
      " -3.69785996e-02  5.22684779e-02  6.65311216e-01 -7.56237913e-02\n",
      " -2.93882803e-02  4.65750132e-02 -2.07112247e-02  8.17381936e-02\n",
      "  3.18926466e-02 -6.05938774e-03 -2.34145665e-01  2.07264826e-01\n",
      "  2.68748291e-02 -2.75100461e-02  5.75702227e-06  3.39331308e-02\n",
      " -1.02703189e-02 -3.34138144e-02  1.79871085e-01  9.34575330e-03\n",
      " -8.74855308e-02 -1.74237522e-02  6.20731713e-02 -3.16820832e-02\n",
      "  7.91034238e-02  9.30206520e-02 -5.56862037e-02  6.59649028e-02\n",
      " -4.74246784e-02 -2.46496068e-01 -7.55199652e-02 -1.53792392e-02\n",
      " -2.49783440e-02 -1.49770585e-01  1.19841505e-01 -2.03551806e-03\n",
      " -4.89038308e-02 -4.85247748e-02  2.85348752e-02  4.05902677e-02\n",
      " -2.73977734e-03 -5.56445525e-03 -6.31061172e-02  6.18458128e-02\n",
      "  4.28220481e-02  8.96524038e-03  3.33182578e-02  3.09614954e-02\n",
      "  4.99975435e-02  1.72729357e-01  8.62563689e-03  4.96766019e-02\n",
      " -8.92889404e-02  4.42282517e-03  7.11294571e-03  6.54528089e-02\n",
      "  5.17706544e-02 -1.54278426e-01 -2.09825346e-03 -8.85456425e-04\n",
      " -6.48070821e-02  8.69504566e-02  8.66348821e-02 -2.53237348e-02\n",
      " -3.16526561e-02  2.88542919e-01  2.24807405e-01 -2.48728268e-02\n",
      " -4.47132403e-02 -7.82424973e-02  5.09331541e-02  8.25736293e-02\n",
      "  3.41910251e-02  1.02186257e-01  1.43144995e-01 -2.33661646e-02\n",
      "  3.45967092e-02 -3.03613113e-02 -1.39112619e-01  3.05587465e-02\n",
      "  6.83481218e-03 -4.39850740e-02 -1.17374952e-01 -6.91175299e-02\n",
      " -4.47527417e-02 -3.25959504e-02 -3.84450276e-02  2.55009541e-02\n",
      "  1.82002984e-02  6.79272720e-02 -7.68406115e-02 -2.47539196e-02\n",
      " -2.89360719e-02 -3.75559461e-02 -7.37622592e-02  9.46551993e-02\n",
      "  1.57810076e-01 -1.55469957e-01  7.88141999e-02  3.16405169e-02\n",
      "  2.17205686e-02  2.19262730e-02  6.28124114e-02  1.14806498e-01\n",
      "  1.36989292e-01 -6.02040162e-04  6.66557491e-04 -2.80053279e-02\n",
      " -1.18440708e-03 -5.65586184e-03  6.71224631e-02  1.00568659e-01\n",
      " -1.64279314e-01  8.65653643e-03  3.96954653e-02 -1.43297161e-01\n",
      " -4.73488855e-02  6.81528053e-02  1.83899948e-02 -8.73701675e-03\n",
      " -7.72313103e-02 -1.46353459e-02 -1.01985546e-02 -2.68701222e-02\n",
      " -5.40211418e-02  1.14790736e-02 -4.76934694e-02 -2.71385451e-01\n",
      "  1.45368351e-01  1.03736407e-02 -7.82356022e-03 -2.05339687e-02\n",
      "  5.88759483e-02  5.55323179e-02  1.18627774e-01  2.15560819e-01\n",
      "  3.60998227e-02]\n"
     ]
    }
   ],
   "source": [
    "alpha=30\n",
    "\n",
    "ww0, bb0 = fit_linreg(X_train, y_train, alpha)\n",
    "\n",
    "print(\"Bias is:\",bb0)\n",
    "print(\"Weights are:\",\"\\n\",ww0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff525d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Support code\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from scipy.linalg import cho_factor, cho_solve\n",
    "\n",
    "def params_unwrap(param_vec, shapes, sizes):\n",
    "    \"\"\"Helper routine for minimize_list\"\"\"\n",
    "    args = []\n",
    "    pos = 0\n",
    "    for i in range(len(shapes)):\n",
    "        sz = sizes[i]\n",
    "        args.append(param_vec[pos:pos+sz].reshape(shapes[i]))\n",
    "        pos += sz\n",
    "    return args\n",
    "\n",
    "\n",
    "def params_wrap(param_list):\n",
    "    \"\"\"Helper routine for minimize_list\"\"\"\n",
    "    param_list = [np.array(x) for x in param_list]\n",
    "    shapes = [x.shape for x in param_list]\n",
    "    sizes = [x.size for x in param_list]\n",
    "    param_vec = np.zeros(sum(sizes))\n",
    "    pos = 0\n",
    "    for param in param_list:\n",
    "        sz = param.size\n",
    "        param_vec[pos:pos+sz] = param.ravel()\n",
    "        pos += sz\n",
    "    unwrap = lambda pvec: params_unwrap(pvec, shapes, sizes)\n",
    "    return param_vec, unwrap\n",
    "\n",
    "\n",
    "def linreg_cost(params, X, yy, alpha):\n",
    "    \"\"\"Regularized least squares cost function and gradients\n",
    "\n",
    "    Can be optimized with minimize_list -- see fit_linreg_gradopt for a\n",
    "    demonstration.\n",
    "\n",
    "    Inputs:\n",
    "    params: tuple (ww, bb): weights ww (D,), bias bb scalar\n",
    "         X: N,D design matrix of input features\n",
    "        yy: N,  real-valued targets\n",
    "     alpha: regularization constant\n",
    "\n",
    "    Outputs: (E, [ww_bar, bb_bar]), cost and gradients\n",
    "    \"\"\"\n",
    "    # Unpack parameters from list\n",
    "    ww, bb = params\n",
    "\n",
    "    # forward computation of error\n",
    "    ff = np.dot(X, ww) + bb\n",
    "    res = ff - yy\n",
    "    E = np.dot(res, res) + alpha*np.dot(ww, ww)\n",
    "\n",
    "    # reverse computation of gradients\n",
    "    ff_bar = 2*res\n",
    "    bb_bar = np.sum(ff_bar)\n",
    "    ww_bar = np.dot(X.T, ff_bar) + 2*alpha*ww\n",
    "\n",
    "    return E, [ww_bar, bb_bar]\n",
    "\n",
    "def minimize_list(cost, init_list, args):\n",
    "    \"\"\"Optimize a list of arrays (wrapper of scipy.optimize.minimize)\n",
    "\n",
    "    The input function \"cost\" should take a list of parameters,\n",
    "    followed by any extra arguments:\n",
    "        cost(init_list, *args)\n",
    "    should return the cost of the initial condition, and a list in the same\n",
    "    format as init_list giving gradients of the cost wrt the parameters.\n",
    "\n",
    "    The options to the optimizer have been hard-coded. You may wish\n",
    "    to change disp to True to get more diagnostics. You may want to\n",
    "    decrease maxiter while debugging. Although please report all results\n",
    "    in Q2-5 using maxiter=500.\n",
    "    \"\"\"\n",
    "    opt = {'maxiter': 500, 'disp': False}\n",
    "    init, unwrap = params_wrap(init_list)\n",
    "    def wrap_cost(vec, *args):\n",
    "        E, params_bar = cost(unwrap(vec), *args)\n",
    "        vec_bar, _ = params_wrap(params_bar)\n",
    "        return E, vec_bar\n",
    "    res = minimize(wrap_cost, init, args, 'L-BFGS-B', jac=True, options=opt)\n",
    "    return unwrap(res.x)\n",
    "\n",
    "\n",
    "def fit_linreg_gradopt(X, yy, alpha):\n",
    "    \"\"\"\n",
    "    fit a regularized linear regression model with gradient opt\n",
    "\n",
    "         ww, bb = fit_linreg_gradopt(X, yy, alpha)\n",
    "\n",
    "     Find weights and bias by using a gradient-based optimizer\n",
    "     (minimize_list) to improve the regularized least squares cost:\n",
    "\n",
    "       np.sum(((np.dot(X,ww) + bb) - yy)**2) + alpha*np.dot(ww,ww)\n",
    "\n",
    "     Inputs:\n",
    "             X N,D design matrix of input features\n",
    "            yy N,  real-valued targets\n",
    "         alpha     scalar regularization constant\n",
    "\n",
    "     Outputs:\n",
    "            ww D,  fitted weights\n",
    "            bb     scalar fitted bias\n",
    "    \"\"\"\n",
    "    D = X.shape[1]\n",
    "    args = (X, yy, alpha)\n",
    "    init = (np.zeros(D), np.array(0))\n",
    "    ww, bb = minimize_list(linreg_cost, init, args)\n",
    "    return ww, bb\n",
    "\n",
    "def logreg_cost(params, X, yy, alpha):\n",
    "    \"\"\"Regularized logistic regression cost function and gradients\n",
    "\n",
    "    Can be optimized with minimize_list -- see fit_linreg_gradopt for a\n",
    "    demonstration of fitting a similar function.\n",
    "\n",
    "    Inputs:\n",
    "    params: tuple (ww, bb): weights ww (D,), bias bb scalar\n",
    "         X: N,D design matrix of input features\n",
    "        yy: N,  real-valued targets\n",
    "     alpha: regularization constant\n",
    "\n",
    "    Outputs: (E, [ww_bar, bb_bar]), cost and gradients\n",
    "    \"\"\"\n",
    "    # Unpack parameters from list\n",
    "    ww, bb = params\n",
    "\n",
    "    # Force targets to be +/- 1\n",
    "    yy = 2*(yy==1) - 1\n",
    "\n",
    "    # forward computation of error\n",
    "    aa = yy*(np.dot(X, ww) + bb)\n",
    "    sigma = 1/(1 + np.exp(-aa))\n",
    "    E = -np.sum(np.log(sigma)) + alpha*np.dot(ww, ww)\n",
    "\n",
    "    # reverse computation of gradients\n",
    "    aa_bar = sigma - 1\n",
    "    bb_bar = np.dot(aa_bar, yy)\n",
    "    ww_bar = np.dot(X.T, yy*aa_bar) + 2*alpha*ww\n",
    "\n",
    "    return E, (ww_bar, bb_bar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1262a835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08983942508643088 \n",
      "\n",
      "[-6.08928794e-02 -1.06109231e-01  7.53199790e-02  2.78058745e-01\n",
      "  2.59650357e-01  1.19767171e-01  1.05065679e-02  2.32640328e-01\n",
      " -3.17410095e-01 -7.79395725e-02 -4.16863910e-02  4.99515149e-02\n",
      " -2.22130095e-02 -2.47116900e-03 -7.34062422e-03  3.76237659e-02\n",
      "  2.67685541e-02  3.28086014e-03 -8.14198810e-02  1.75633257e-01\n",
      "  7.92432171e-02  2.86749071e-02 -4.09596766e-02 -6.22108255e-03\n",
      " -6.01534646e-02 -5.63056508e-02 -1.47844013e-02 -4.10930801e-01\n",
      "  3.40022329e-01  2.63335827e-01 -1.35872609e-02  1.66221922e-03\n",
      " -9.11711030e-03 -7.00848604e-03  6.51838705e-02 -1.75613930e-01\n",
      " -1.79693155e-01  2.20564235e-01 -9.07051362e-02 -3.30882147e-02\n",
      " -1.80930149e-01  1.91309155e-02 -1.74997057e-01  5.31853600e-02\n",
      " -1.40982937e-01 -2.01732057e-02 -2.04407470e-02  2.17917797e-02\n",
      " -4.51780456e-01  8.75602083e-02 -9.25508965e-02 -3.47882886e-03\n",
      " -7.86813533e-02 -5.90120793e-02 -3.59756491e-02 -8.00154974e-02\n",
      "  5.00354796e-02 -1.32535849e-01  1.16684096e-01 -7.66211791e-02\n",
      "  6.28223777e-02 -1.36229313e-02 -1.24330837e-01 -1.11965530e-01\n",
      " -1.05871183e-01 -1.08969216e-01  1.02886867e-01  1.28952656e-01\n",
      "  1.61689458e-02  8.28466846e-02  1.21645961e-01  2.34963112e-02\n",
      "  6.00149070e-02  5.35866904e-02 -1.58918008e-01 -2.10641364e-01\n",
      "  5.54141259e-02 -4.45863056e-02 -6.35148087e-02  2.30299632e-02\n",
      " -3.48423724e-03 -9.14796883e-02 -1.77799288e-02  1.81631473e-01\n",
      " -1.10361931e-02 -8.83894649e-02  6.53633872e-02 -1.32857595e-01\n",
      "  6.07991204e-02 -4.46443952e-02  6.61710232e-03  3.18716000e-02\n",
      " -1.31928383e-01  3.15312011e-01  4.07418423e-01  2.93765935e-01\n",
      " -8.36635563e-02 -1.91141343e-03  3.90614945e-02  1.23188395e-02\n",
      "  3.61823948e-02  3.92709528e-02  2.22194561e-01  1.63853835e-02\n",
      " -8.40181281e-02 -9.78930805e-02 -1.87805464e-01  3.98960900e-03\n",
      " -3.28181029e-02  6.24754977e-02  2.41359743e-01  1.53132933e-01\n",
      "  1.17871715e-01  5.04771145e-03 -2.77936284e-01 -2.24971005e-01\n",
      " -2.09794412e-01  1.01600861e-01  4.10031287e-02 -5.67779475e-02\n",
      " -7.22616851e-03 -1.30914257e-02 -3.29799233e-02 -1.63048724e-02\n",
      " -1.13703997e-01  4.67278344e-02 -9.78534472e-02  6.50122713e-02\n",
      "  2.24133488e-01 -1.09481916e-01  3.08966960e-02 -1.91934583e-01\n",
      " -9.05709762e-02 -1.42310729e-01 -5.37337873e-02  8.61357226e-02\n",
      " -2.65850257e-02 -6.77776969e-02  6.67895029e-02  1.31791402e-01\n",
      "  2.32979093e-02 -7.07463407e-02 -8.02807016e-02 -5.37762761e-02\n",
      "  1.21732669e-01 -4.13503869e-02 -7.21680826e-02 -9.11027951e-02\n",
      " -9.66936396e-02  6.00376469e-02 -1.89070371e-02  1.31356759e-02\n",
      " -8.06093542e-02 -5.16988568e-03  2.04767679e-02  8.26309211e-02\n",
      " -1.48418297e-02  3.94584807e-02 -1.56678148e-03 -7.56326185e-02\n",
      "  2.00845756e-02 -5.56619139e-02  7.09943147e-02 -1.12255194e-02\n",
      " -3.29669750e-01  1.67421587e-01 -1.17864799e-01 -8.46771638e-03\n",
      " -1.52475024e-02 -1.90052668e-04 -1.04653438e-01  1.23940147e-02\n",
      " -1.52037061e-01 -8.98563978e-02  1.81387967e-01 -1.12574496e-01\n",
      " -2.80129229e-02  1.00026351e-01  5.61223944e-02 -7.11505360e-03\n",
      " -5.68420193e-03 -7.74719007e-02 -1.95968771e-01  1.63640032e-03\n",
      "  1.35094037e-01  1.49504877e-02 -4.16632971e-02  2.89062827e-02\n",
      " -1.08661360e-01 -6.31554403e-02  8.79593748e-03 -3.16315189e-01\n",
      " -5.92186461e-02 -1.70756520e-02 -4.84292061e-02 -1.54408150e-02\n",
      " -1.27732925e-02  2.56160160e-02 -5.11524069e-02  7.80299936e-02\n",
      " -9.56880988e-03 -5.67617019e-02 -4.59576807e-02 -9.93349883e-02\n",
      "  9.34475794e-02  1.38667597e-02  5.56113997e-02 -1.03531343e-01\n",
      " -2.93211618e-03 -1.97441247e-01 -1.15997124e-01  2.44541663e-02\n",
      " -7.51702534e-02  2.95928810e-02  1.59707577e-01 -2.55645697e-02\n",
      " -7.06091726e-02 -1.18534186e-01 -1.28838357e-01 -1.90195067e-01\n",
      " -1.15565059e-01  1.72652918e-01 -1.35073240e-01 -8.56864615e-04\n",
      " -1.75883560e-01  4.37668180e-02 -3.36673225e-02  3.78182948e-02\n",
      " -1.00931194e-01 -1.07940908e-01  1.38558665e-01 -1.08572560e-01\n",
      " -1.79511559e-03  1.44355410e-02 -1.61822140e-01  2.10474878e-02\n",
      " -3.69732171e-02  5.22699748e-02  6.64909913e-01 -7.47038569e-02\n",
      " -2.93728165e-02  4.65758235e-02 -2.07228618e-02  8.17312169e-02\n",
      "  3.18947744e-02 -6.11982123e-03 -2.34721468e-01  2.06611926e-01\n",
      "  2.68506259e-02 -2.75091743e-02 -3.48280798e-06  3.39604431e-02\n",
      " -1.02779070e-02 -3.33938149e-02  1.79777128e-01  9.48707445e-03\n",
      " -8.74980090e-02 -1.74479283e-02  6.20921204e-02 -3.16867501e-02\n",
      "  7.90929129e-02  9.30273513e-02 -5.54476368e-02  6.58264757e-02\n",
      " -4.73942402e-02 -2.46474211e-01 -7.55331041e-02 -1.53875016e-02\n",
      " -2.50063552e-02 -1.49699414e-01  1.20108136e-01 -4.77789539e-03\n",
      " -4.88811875e-02 -4.85334881e-02  2.85253271e-02  4.05814574e-02\n",
      " -2.72337403e-03 -5.66845176e-03 -6.32060415e-02  6.18328789e-02\n",
      "  4.28298591e-02  8.97347269e-03  3.32842948e-02  3.09666006e-02\n",
      "  5.00507020e-02  1.72908221e-01  8.79738273e-03  4.96899373e-02\n",
      " -8.92533531e-02  4.40648834e-03  7.12893697e-03  6.54679468e-02\n",
      "  5.17919811e-02 -1.55278377e-01 -1.87237624e-03 -9.01853905e-04\n",
      " -6.48358705e-02  8.69620364e-02  8.66368357e-02 -2.53344188e-02\n",
      " -3.16352275e-02  2.88521407e-01  2.24127167e-01 -2.48469962e-02\n",
      " -4.47598621e-02 -7.82063149e-02  5.08826866e-02  8.25922441e-02\n",
      "  3.41387810e-02  1.02239017e-01  1.43418218e-01 -2.33768788e-02\n",
      "  3.45930298e-02 -3.03579448e-02 -1.39134993e-01  3.05424015e-02\n",
      "  6.88297296e-03 -4.40229366e-02 -1.17080671e-01 -6.91131214e-02\n",
      " -4.47953396e-02 -3.25878095e-02 -3.84207327e-02  2.55475152e-02\n",
      "  1.82217783e-02  6.74735465e-02 -7.71769105e-02 -2.47607871e-02\n",
      " -2.89177010e-02 -3.75375113e-02 -7.37685252e-02  9.46366055e-02\n",
      "  1.58012317e-01 -1.55653204e-01  7.90452110e-02  3.16318438e-02\n",
      "  2.17404690e-02  2.19214597e-02  6.29257190e-02  1.14764089e-01\n",
      "  1.36851365e-01 -5.87864813e-04  6.81254192e-04 -2.80062532e-02\n",
      " -1.20040957e-03 -5.66737185e-03  6.71045887e-02  1.00646611e-01\n",
      " -1.64542055e-01  8.64802932e-03  3.97164979e-02 -1.43265415e-01\n",
      " -4.73988885e-02  6.81827024e-02  1.83255032e-02 -8.76705826e-03\n",
      " -7.74902347e-02 -1.46460512e-02 -1.02048296e-02 -2.69252301e-02\n",
      " -5.39942944e-02  1.14491401e-02 -4.77314306e-02 -2.71609767e-01\n",
      "  1.45860808e-01  1.03808349e-02 -7.78729778e-03 -2.05181519e-02\n",
      "  5.89063311e-02  5.55299237e-02  1.18652613e-01  2.15888521e-01\n",
      "  3.53028113e-02]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(373,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ww1,bb1 = fit_linreg_gradopt(X_train, y_train, 30)\n",
    "print(bb1,\"\\n\")\n",
    "print(ww1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc55291c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(pred,yy):\n",
    "    return np.sqrt(np.mean((pred-yy)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99418f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root means square for training set(using least square method): 0.3567565397204054\n",
      "Root means square for validation set(using least square method): 0.4230521968394701\n"
     ]
    }
   ],
   "source": [
    "## for least square method\n",
    "pred1_train = np.dot(X_train,ww0)+bb0\n",
    "pred2_val = np.dot(X_val,ww0)+bb0\n",
    "print(\"Root means square for training set(using least square method):\",rmse(pred1_train, y_train))\n",
    "print(\"Root means square for validation set(using least square method):\",rmse(pred2_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b126c2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root means square for training set(using least square method): 0.3567569385517838\n",
      "Root means square for validation set(using least square method): 0.4230540100048547\n"
     ]
    }
   ],
   "source": [
    "# for gradient method\n",
    "pred1_train = np.dot(X_train,ww1)+bb1\n",
    "pred2_val = np.dot(X_val,ww1)+bb1\n",
    "print(\"Root means square for training set(using least square method):\",rmse(pred1_train, y_train))\n",
    "print(\"Root means square for validation set(using least square method):\",rmse(pred2_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab04f282",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################  3rd question ################################\n",
    "def sigmoid(a):\n",
    "    return 1/(1+np.exp(-a))\n",
    "\n",
    "\n",
    "K = 20 # number of thresholded classification problems to fit\n",
    "mx = np.max(y_train); mn = np.min(y_train); hh = (mx-mn)/(K+1)\n",
    "thresholds = np.linspace(mn+hh, mx-hh, num=K, endpoint=True)\n",
    "\n",
    "par=[ww0,bb0]\n",
    "w_fit2= np.array([[0]* (len(X_train[1])+1)] * K)\n",
    "for kk in range(K):\n",
    "    labels = y_train > thresholds[kk]\n",
    "    ww2, bb2 = logreg_cost(par, X_train, labels, 30)[1]\n",
    "    w_fit2[kk,:] = np.concatenate((np.array([bb2]),np.array(ww2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bed9bef1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 373)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bb2 = w_fit2[:,0]\n",
    "ww2 = w_fit2[:,1:]\n",
    "ww2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e1ca273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 20)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dt/crfvypl96mz6l_hxsgxvyspw0000gn/T/ipykernel_1008/1320994693.py:3: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-a))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(40754, 20)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_new = sigmoid(np.dot(X_train,np.transpose(ww2)))\n",
    "new_ww, new_bb = fit_linreg(X_train_new, y_train, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "546bbdc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dt/crfvypl96mz6l_hxsgxvyspw0000gn/T/ipykernel_1008/1320994693.py:3: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-a))\n"
     ]
    }
   ],
   "source": [
    "X_val_new = sigmoid(np.dot(X_val,np.transpose(ww2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cc0e8f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root means square for training set: 0.8435958826264376\n",
      "Root means square for validation set: 0.8707488882316101\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pred1_train = np.dot(X_train_new, new_ww) + new_bb\n",
    "pred2_val = np.dot(X_val_new, new_ww) + new_bb\n",
    "print(\"Root means square for training set:\",rmse(pred1_train, y_train))\n",
    "print(\"Root means square for validation set:\",rmse(pred2_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bafe6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b003ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a40641d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7759510",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e349d1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train>thresholds[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608d25ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
